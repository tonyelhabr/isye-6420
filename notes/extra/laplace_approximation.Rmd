
#### Laplace Approximation [^4]

<ref>
"I’ll demonstrate the use of quadratic approximation with a model for an individual team’s score differential. The first step is to gather the score differential for each team in each season...
for NFL seasons from 2009 to 2017.
</ref>


```{r}
# Load the tidyverse
# install.packages("tidyverse")
library(tidyverse)

# Use the nflWAR package to get summaries of team performances for each season,
# first install devtools if you don't have then install nflWAR:
# install.packages("devtools")
# devtools::install_github("ryurko/nflWAR")
library(nflWAR)

# Use the get_season_summary() function to create a dataset that has a row
# for team-season with a column for their score differential from 2009 to 2017:
team_summary_df <- get_season_summary(2009:2017)
```

<ref>
Before going into the regression example with a predictor, it’s worthwhile to first demonstrate quadratic approximation by just modeling the score differential with a Gaussian.
</ref>

```{r}
# Create a histogram displaying the score differential distribution using
# the the density instead so it's consistent for later: (also why I'm assigning
# it to a variable name, so we can add our approximated posterior layer later)
score_diff_hist <- team_summary_df %>%
  ggplot(aes(x = Total_Score_Diff)) +
  geom_histogram(aes(y = ..density..), bins = 20) + theme_bw() + 
  scale_x_continuous(limits = c(-300, 300)) +
  # Always label:
  labs(x = "Score differential", y = "Density",
       title = "Distribution of individual team score differential in each season from 2009-17 ",
       caption = "Data accessed with nflscrapR") +
  # Just some cosmetic settings: (really should do a custom theme at the 
  # beginning instead of repeating this)
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(size = 12),
        plot.title = element_text(size = 14))

# Display the plot
score_diff_hist
```

<ref>
"From this histogram we have support for using a Gaussian, since it’s roughly symmetric and unimodal.<br/><br/>
We assume then that the general model for an observed score differential $S_i$, where $i = 1, \dots, 288$
is the index for an observed team-season score differential, follows a Gaussian distribution:
</ref>

$$
\begin{array}{c}
S_i \sim \mathcal{N}(\mu, \sigma^2)
\end{array}
$$

<ref>
"Because we’re approaching this problem from a Bayesian perspective, we really want to consider an infinite number of possible Guassians based on all of the possible combinations for our parameters $\mu$ and $\sigma$. Of course we’re not going to actually look at every possible value for these parameters, Laplace approximation will accomplish what we need. But, ... we’re interested in the entire posterior distribution - which in this case is a posterior distribution of distributions. So for a full model of score differential, we need priors for both  $\mu$ and $\sigma$.
</ref>

$$
\begin{array}{c}
S_i \sim \mathcal{N}(\mu, \sigma^2) \\
\mu \sim \mathcal{N}(0, 100^2) \\
\sigma \sim \mathcal{U}(0,200)
\end{array}
$$
<ref>
"This is a really naive model, just to demonstrate Laplace approximation. Next we’ll implement the [some] code ... which uses the `optim()` function in R to find the mode of the posterior and also returns the Hessian matrix for the curvature (following the observed information definition above). For ease, we’ll ... [use] the `{mvtnorm}` package for sampling from the approximated posterior. (This sampling isn’t really necessary but it’s good practice to start sampling from the posterior). We’re working with two parameters here, so the resulting posterior approximation is a multivariate Gaussian, hence why we use the `rmvnorm()` function since we’re not only working with each parameter’s mean and variance, but their covariance as well. (In this example the covariance is essentially 0, but that won’t typically be the case).
</ref>

```{r}
# Function to perform simple Laplace approximation
# @param log_posterior_n Function that returns the log posterior given a vector
# of parameter values and observed data.
# @param init_points Vector of starting values for parameters.
# @param n_samples Number of samples to draw from resulting approximated 
# posterior distribution for then visualizing.
# @param ... Any additional arguments passed to the log_posterior_fun during 
# the optimization.
# @return Samples from approximated posterior distribution.
laplace_approx <- function(log_posterior_fn, init_points, n_samples, ...) {
  
  # Use the optim function with the input starting points and function to
  # evaluate the log posterior to find the mode and curvature with hessian = TRUE
  # (note that using fnscale = -1 means to maximize). We use the ... here to 
  # allow for flexible name of the data in log_posterior_fn:
  fit <- optim(init_points, log_posterior_fn, 
               # Use the same quasi-Newton optimization method as McElreath's
               # map function in his rethinking package:
               method = "BFGS",
               # using fnscale = -1 means to maximize
               control = list(fnscale = -1), 
               # need the hessian for the curvature
               hessian = TRUE, 
               # additional arguments for the function we're optimizing
               ...)
  
  # Store the mean values for the parameters and curvature to then generate
  # samples from the approximated normal posterior given the number of samples
  # for both of the parameters, returning as a data frame:
  param_mean <- fit$par
  # inverse of the negative hessian for the covariance - same as the use 
  # of the observed information in the intro
  param_cov_mat <- solve(-fit$hessian)
  # sample from the resulting joint posterior to get posterior distributions
  # for each parameter:
  mvtnorm::rmvnorm(n_samples, param_mean, param_cov_mat) %>%
    data.frame()
}

# Function to calculate log posterior for simple score differential model
# @param params Vector of parameter values that are named "mu" and "sigma".
# @param score_diff_values Vector of observed score differential values
score_diff_model <- function(params, score_diff_values) {
  
  # Log likelihood:
  sum(dnorm(score_diff_values, params["mu"], params["sigma"], log = TRUE)) +
    # plus the log priors results in log posterior:
    dnorm(params["mu"], 0, 100, log = TRUE) + 
    dunif(params["sigma"], 0, 200, log = TRUE)

}
```

<ref>
"With these functions we’ll now approximate the posterior, considering very naive starting values for our optimization with $\mu = 200$ and $\sigma = 10$. (This is just to demonstrate, I have no reason for choosing these values).
</ref>

```{r}
init_samples <- laplace_approx(log_posterior_fn = score_diff_model,
                               init_points = c(mu = 200, sigma = 10),
                               n_samples = 10000,
                               # Specify the data for the optimization!
                               score_diff_values = team_summary_df$Total_Score_Diff)
```


<ref>
"We can easily view the joint distribution of our posterior samples with their respective marginals on the side.
</ref>

```{r}
# install.packages("cowplot")
library(cowplot)

# First the joint distribution plot with points for the values:
joint_plot <- ggplot(init_samples, aes(x = mu, y = sigma)) + 
  geom_point(alpha = .3, color = "darkblue")  +
  labs(title = "Posterior distribution for model parameters with marginals") +
  theme_bw() +
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(size = 12),
        plot.title = element_text(size = 16))

# Marginal density for mu along x-axis using the cowplot function axis_canvas:
mu_dens <- axis_canvas(joint_plot, axis = "x") +
  geom_density(data = init_samples, aes(x = mu), fill = "darkblue",
               alpha = 0.5, size = .2)

# Same thing for sigma but along y and with coord_flip = TRUE to make it vertical:
sigma_dens <- axis_canvas(joint_plot, axis = "y", coord_flip = TRUE) +
  geom_density(data = init_samples, aes(x = sigma), fill = "darkblue",
               alpha = 0.5, size = .2) +
  coord_flip()

# Now generate by adding these objects to the main plot:
# Need grid:
# install.packages("grid")
joint_plot_1 <- insert_xaxis_grob(joint_plot, mu_dens, 
                                  grid::unit(.2, "null"), position = "top")
joint_plot_2 <- insert_yaxis_grob(joint_plot_1, sigma_dens, 
                                  grid::unit(.2, "null"), position = "right")
ggdraw(joint_plot_2)
```


<ref>
"From this we can clearly see the Gaussian approximations for both of our parameters with the distribution of 
$\mu$ centered around 0 and the distribution for $\sigma$ centered around 100. Remember this represents the distributions for the parameters of our score differential model. So sampling from this posterior means we are generating different Gaussian distributions for the score differential. ... [T]he code below uses these 10000 values from `init_samples()` for each parameter, and then samples 10000 values from distributions using these combinations of values to give us our approximate score differential distribution.
</ref>
 
```{r}
# R is vectorized so can just give it the vector of values for the Gaussian
# distribution parameters to create the simulated score-diff distribution:
sim_score_diff_data <- data.frame(Total_Score_Diff = 
                                    rnorm(10000, mean = init_samples$mu,
                                          sd = init_samples$sigma))

# Create the histogram from before but now add this posterior density on top:
score_diff_hist + geom_density(data = sim_score_diff_data,
                               aes(x = Total_Score_Diff), 
                               fill = NA, color = "darkblue")
```

<ref>
"Not bad, but then again this was a pretty easy example. You could also use the grid approximation from the last post to accomplish the same task but it starts to be annoying to work with even with just two parameters, quickly becoming unbearable with more and more parameters.
</ref>


#### Bayesian Linear Regression with Laplace Approximation [^4]

<ref>
"Ok, so we’ve seen how easy it is to use Laplace approximation for modeling score differential… but that didn’t tell us anything interesting. What we’re really interested in, is the relationship between some predictor variable(s) and a team’s score differential. For instance, how is a team’s passing performance related to their score differential? What about their pass defense? Is this more or less related to score differential than rushing?
</ref>

<ref>
"... [W]e’re now going use both efficiency differentials as predictors. Remember, all we need to fully describe a Gaussian distribution is the mean and variance. So in order to incorporate these into the our model we’ll define the mean, $\mu$ as a function of both predictors explicitly. For simplicity we’ll return to using $i$ to denote a single combination of team $t$ and season $y$ for the 286 team-season combinations that we have, and we’ll also let $P_i$ and $R_i$ be representations of passing and running efficiency:
</ref>

$$
\begin{array}{c}
S_i \sim \text{N}(\mu_i, \sigma^2) \\
\mu_i = \alpha + \beta_{P}P_i + \beta_R R_i \\
\alpha \sim \text{N}(0, 100^2) \\
\beta_P \sim \text{N}(0, 100^2) \\
\beta_R \sim \text{N}(0, 100^2) \\
\sigma \sim \text{Uniform}(0,200)
\end{array}
$$


<ref>
"Now the mean of the score differential distribution _depends on the predictors_
as denoted with the subscript $i$. By denoting the relationship between $\mu_i$ 
and each of the predictors with an $=$ sign, we're saying that $\mu_i$ is no 
longer a parameter, its just a function of our actual parameters of interest.
The parameters we really care about here are $\alpha$, $\beta_P$, and $\beta_R$.
Of course all of this should look familiar if you've seen [linear regression](https://en.wikipedia.org/wiki/Linear_regression)
before. The $\alpha$ is our model's intercept, representing the _expected_ score
differential for when both the passing and rushing efficiency differentials 
are 0. And each of the $\beta$s represent the change in _expected_ score 
differential when the pass/rush efficiency differential increases by 1 unit, after
accounting for the other type of efficiency differential (rush/pass). So each of
these are given priors (along with $\sigma$ again), and we don't need to specify 
a prior for $\mu_i$ since it is explained completely by these parameters. These
priors for now are pretty weak, but both $\beta$ priors centered at 0 are just
saying, "I don't know if there's a relationship between these effiency 
differentials and score differential, good or bad are equally likely.
</ref>

<ref>
"To actually approximate this model..."
</ref>
