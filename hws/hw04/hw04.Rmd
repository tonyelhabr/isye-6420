---
title: 'ISYE 6420: Homework 4'
author: 'aelhabr3'
output:
  html_document:
    css: ../styles_hw.css
    theme: cosmo
    highlight: haddock
    toc: false
---

```{r setup, include=F, cache=F}
knitr::opts_knit$set(root.dir = here::here())
knitr::opts_chunk$set(
  # rows.print = 25,
  # rows.print = 25,
  echo = TRUE,
  # cache = FALSE,
  cache = TRUE,
  include = TRUE,
  fig.show = 'asis',
  fig.align = 'center',
  fig.width = 8,
  # size = "small",
  # fig.height = 5,
  # fig.width = 5,
  # out.width = 5,
  fig.asp = 0.75,
  warning = FALSE,
  message = FALSE
)
```

```{r postprocess, include=F, echo=F, cache=F}
.path_sans_ext <- file.path('hw4')
.path_rmd <- paste0(.path_sans_ext, '.Rmd')
# spelling::spell_check_files(.path_rmd)
```

```{r setup-1, include=F, echo=F, eval=T}
library(tidyverse)
format_num <- function(x, digits = 4) {
  fmt <- sprintf('%%.%df', digits)
  sprintf(fmt, x)
}
```

```{r pdf-convert-1, include=F, echo=F, eval=F}
convert_page_to_lines <- function(page) {
  page %>%
    str_split('\\n') %>%
    purrr::map(str_squish) %>%
    unlist() %>%
    enframe(name = 'idx_line', value = 'line')
}

unnest_pages <- function(pages) {
  pages %>%
    mutate(lines = purrr::map(page, convert_page_to_lines)) %>%
    select(-page) %>%
    unnest(lines)
}
path_hw <- 'Homework4f.pdf'
pages <- 
  path_hw %>% 
  tibble(path = .) %>% 
  mutate(
    page = purrr::map(path, ~pdftools::pdf_text(.))
  ) %>%
  select(-path)
lines <- pages %>% unnest_pages()
lines
lines_text <-
  lines %>%
  pull(line) %>% 
  paste(collapse = ' ', sep = '')
lines_text
```

# 1. Metropolis: The Bounded Normal Mean.

## Instructions

<instructions>
Suppose that we have information that the normal mean $\theta$ is bounded between $−m$ and $m$, for some known number $m$. In this case it is natural to elicit a prior on $\theta$ with the support on interval $[−m, m]$. A prior with interesting theoretical properties supported on $[−m, m$] is Bickel-Levit prior, 
</instructions>

$$
\begin{array}{c}
\pi(\theta)=\frac{1}{m} \cos ^{2}\left( \frac{\pi \theta}{2 m}\right), \quad -m \leq \theta \leq m.
\end{array}
$$

<instructions>
Assume that a sample $[−2, −3, 4, −7, 0, 4]$ is observed from normal distribution 
</instructions>

<instructions>
with a known precision $\tau = 1/4$. Assume also that the prior on $\theta$ is Bickel-Levit, with $m = 2$. This combination likelihood/prior does not result in an explicit posterior (in terms of elementary functions). Construct a Metropolis algorithm that will sample from the posterior of $\theta$.<br/><br/>
(a) Simulate 10,000 observations from the posterior, after discarding first 500 observations (burn-in), and plot the histogram of the posterior.<br/>
(b) Find Bayes estimator of $\theta$, and 95% equitailed Credible Set based on the simulated observations.<br/><br/>
Suggestions:<br/>
(i) Take uniform distribution on $[−m, m]$ as a proposal distribution since it is easy to sample from. This is an independence proposal, the proposed $\theta'$ does not depend on the current value of the chain, $\theta$.<br/>
(ii) You will need to calculate $\sum_{i=1}^n (y_i − \theta)^2$ for current $\theta$ and $\sum_{i=1}^n i=1 (y_i − \theta')^2$ for the proposed $\theta'$, prior to calculating the Metropolis ratio.
</instructions>


$$
\begin{array}{c}
f(y|\theta) \propto \sqrt{\tau} \text{ exp } \{ \frac{\tau}{2} (y - \theta) ^2 \},
\end{array}
$$

## Response

### a

As noted in the instructions, the posterior cannot be written easily in terms of "elementary" functions.
Alternatively, the Metropolis algorithm can be used to approximate a posterior distribution.

First, let's construct the target density distribution. This is proportional to the posterior, which is the product of the prior and the likelihood. Using the Bickel-Levit prior $\pi(\theta)$ with $m = 2$ and the likelihood distribution $f(y|\theta)$ with $\tau = \frac{1}{4}$, <hide>and $\bar{y} = \frac{1}{n} \sum_{i=1}^n} y_i = \frac{1}{6} (-6) = -1$</hide>, we formulate the target density function as follows.

$$
\begin{array}{rcl}
\text{ posterior } & \propto & \text{ prior } \times \text{ likelihood } \\
\pi(y|\theta) & \propto & \pi(\theta) \times f(y|\theta) \\
& = & (\frac{1}{(2)} \cos ^{2}\left( \frac{\pi \theta}{2 (2)}\right)) (\sqrt{(1/4)} \text{ exp } \{ \frac{(1/4)}{2} (y - \theta) ^2 \}) \\
\end{array}
$$

(This is still a relatively complex expression, but code can help us solve for given values of $\theta$ and $\tau$.)

Using the suggestion and given $m=2$, we define the proposal density as $\mathcal{U}(-2, 2)$.

Then we have the acceptance probability $\rho$ as


$$
\begin{array}{c}
\rho\left(\theta_{n}, \theta^{\prime}\right)=
\min \left\{1, \frac{\pi\left(\theta^{\prime}\right)}{\pi\left(\theta_{n}\right)} \frac{q\left(\theta_{n} | \theta^{\prime}\right)}{q\left(\theta^{\prime} | \theta_{n}\right)}\right\}
=\min \left\{1, z \right\}.
\end{array}
$$

Note that we implement the Metropolis algorithm in the code as follows.

1. Start with arbitrary $\theta$. (Here, 0 is a fine choice.) [^1]

2. At stage $n$, generate proposal $\theta^{\prime}$ from $q(y|x_n)$.

3. Define

$$
\theta_{n+1} = \begin{cases}
\theta^{\prime} & \text{ with probability } & \rho(\theta_{n}, \theta^{\prime}), \\
\theta_{n} & \text{ with probability } &  1 - \rho(\theta_{n}, \theta^{\prime}).
\end{cases}
$$

(Generate $U = \mathcal{U}(0, 1)$ and accept proposal $\theta^{\prime}$ if $U \leq \rho(\theta_{n}, \theta^{\prime})$.)

4. Increment $n$ and go to Step 2.

[^1]: We don't really have an idea of the form of the support of the target; otherwise, we would try to initialize $\theta$ to be like the target.

See the code below for the implementation. (Note that `theta_prop` below represents $\theta^{\prime}$ in the formulations above.)

```{r q1-a-0, include=F, echo=F, eval=F}
# f_likelihood_simple <- function(theta) {
#   exp(theta^2 / 8)
# }
# dbl <- function(theta, m) {
#   theta <- punif(theta, min = -m, max = m)
#   bl(theta, m)
# }
# 
# rbl <- function(n, m) {
#   theta <- runif(n = n, min = -m, max = m)
#   bl(theta, m)
# }

# idx_final <- (n_burnin + 1):n_mcmc
# # Or?
# n_mcmc <- n_mcmc + n_burnin
# idx_final <- (n_burnin + 1):n_mcmc

# Data.
# ~~These are used to calculate the posterior likelihood, but not for MCMC.~~
# y <- ''

```

```{r q1-a-1, include=T, echo=T, eval=T}
set.seed(42)
library(tidyverse)

# Data, constants, hyperparameters, and helper functions.
y <- c(-2, -3, 4, -7, 0, 4)

n_mcmc_q1 <- 10000L
n_burnin_q1 <- 500L

m <- 2
tau <- 1 / 4


.f_likelihood_1 <- function(y, theta, tau) {
  mu <- mean(y)
  sqrt(tau) * exp(-0.5 * tau * (y - theta)^2)
}

.f_likelihood_2 <- function(y, theta, tau) {
  sqrt(tau) * exp(-0.5 * tau * sum(y - theta)^2)
}

bl <- function(theta, m) {
  stopifnot(theta <= m || theta >= -m)
  (1 / m) * (cos((pi * theta) / (2 * m))) ^ 2
}

f_prior <- function(theta, .m = m) {
  bl(theta, m = .m)
}

f_proposal <- function(.n = 1, .m = m) { 
  runif(n = .n, min = -.m, max = .m) 
}

f_likelihood_1 <- function(theta, .y = y, .tau = tau) {
  sum(.f_likelihood_1(theta, y = y, tau = tau))
}

f_likelihood_2 <- function(theta, .y = y, .tau = tau) {
  .f_likelihood_2(theta, y = y, tau = tau)
}

f_likelihood <- f_likelihood_2

is_likeinteger <- function(x, tol = .Machine$double.eps^0.5) {
  abs(x - round(x)) < tol
}

```

```{r q1-a-2-debug-1, include=F, echo=F, eval=F}
.x <- seq(-2, 2, by = 0.01)
qsave <- function(x, nm = deparse(substitute(x))) {
  ggplot2::ggsave(
    filename = sprintf('%s.png', nm),
    plot = x,
    units = 'in', 
    width = 7, 
    height = 5
  )
}
viz_prior <- tibble(x = .x, prior = f_prior(.x)) %>% qplot(x, prior, data = .)
viz_prior
qsave(viz_prior)
viz_prop <- tibble(x = .x, prop = f_proposal(.x)) %>% qplot(x, prop, data = .)
viz_prop
qsave(viz_prop)
viz_likl <- tibble(x = .x, likl = purrr::map_dbl(.x, f_likelihood)) %>% qplot(x, likl, data = .)
viz_likl
qsave(viz_likl)
tibble(x = .x, likl = purrr::map_dbl(.x, ~.f_likelihood_2(.x, y = -1, tau = tau))) %>% 
  qplot(x, likl, data = .)
tibble(x = .x, likl = purrr::map_dbl(.x, ~.f_likelihood_1(.x, y = -1, tau = tau))) %>% 
  qplot(x, likl, data = .)
```


```{r q1-a-2-debug-2, include=F, echo=F, eval=F}
.rnorm_int <- function(...) {
  as.integer(rnorm(...))
}

n_obs <- length(y)

rnorm_int <- function(n = n_obs, mean = 0, sd = 4) {
  # sort(.rnorm_int(n = n, mean = mean, sd = sd))
  .rnorm_int(n = n, mean = mean, sd = sd)
}

.seq <- c(1:9 / 10, 1:2 / 1) %>% sort()
# .y_1 <- rnorm_int()
.y_1 <- c(-3, -4, 0, 1, 7, 0) # Just a manual emulation of `.y`.
# set.seed(1)
.y_2 <- rnorm_int()
mean(.y_2)
sd(.y_2)
# .y_2 <- c(3, -4, 1, -6, 1, 5)
res_sim <-
  c(-.seq, 0, .seq) %>% 
  sort() %>% 
  tibble(theta = .) %>% 
  mutate(idx = row_number()) %>% 
  select(idx, everything()) %>% 
  group_by(idx) %>% 
  mutate(
    # pi_0 = bl(theta, m = 4),
    prior = f_prior(theta),
    # f_1 = sum(.f_1(theta)), 
    # f = f_likelihood(theta),
    # f = purrr::map_dbl(theta, .f_likelihood_2, y = y, tau = tau),
    # f_a_1 = .f_likelihood_1(theta, y = y, tau = tau),
    # f_0 = .f_likelihood_2(theta, y = sort(y), tau = tau),
    # f_b_1 = .f_likelihood_1(theta, y = mean(.y_1), tau = tau),
    # f_c_1 = .f_likelihood_1(theta, y = mean(.y_2), tau = tau),
    likl_actual_2 = .f_likelihood_2(theta, y = y, tau = tau),
    likl_sim1_2 = .f_likelihood_2(theta, y = .y_1, tau = tau),
    likl_sim2_2 = .f_likelihood_2(theta, y = .y_2, tau = tau)
  ) %>% 
  ungroup()
res_sim
viz_sim <-
  res_sim %>% 
  gather(key = 'key', value = 'value', -matches('idx|theta')) %>% 
  ggplot() +
  aes(x = theta, y = value, color = key) +
  geom_point() +
  geom_line() +
  # scale_x_continuous(trans = 'reciprocal')
  xlim(-1.1, 1.1) +
  theme_light()
viz_sim
qsave(viz_sim)
```

```{r q1-a-4, include=T, echo=T, eval=T}
# Main function.
.do_mcmc_mh_q1 <- function(..., n_mcmc = 10000L, n_burnin = 1000L, theta_init = 0) {
  
  # n_mcmc = n_mcmc_q1; n_burnin = n_burnin_q1; theta_init = 0
  
  stopifnot(is_likeinteger(n_mcmc))
  if(!is.null(n_burnin)) {
    stopifnot(is_likeinteger(n_burnin))
    stopifnot(n_burnin < n_mcmc)
  }
  cols_mat_mcmc <-
    c(
      'theta_current', 
      'theta_prop',
      'q_current',
      'q_prop',
      'pi_current',
      'pi_prop',
      'ratio_num',
      'ratio_den',
      'ratio', 
      'rho',
      'u', 
      'is_accepted'
    )
  mat_mcmc <- matrix(nrow = n_mcmc, ncol = length(cols_mat_mcmc))
  colnames(mat_mcmc) <- cols_mat_mcmc
  
  # Step 1 achieved with `.theta_init`.
  theta_current <- theta_init
  # theta_current <- f_proposal()
  # theta_current <- 1
  for (i in 1:n_mcmc) {
    #  i <- 1
    
    # Step 2.
    theta_prop <- f_proposal()
    
    # Step 3.
    q_current <- f_prior(theta = theta_current)
    q_prop <- f_prior(theta = theta_prop)
    
    pi_current <- f_likelihood_2(theta = theta_current)
    pi_prop <- f_likelihood_2(theta = theta_prop)
    
    # ratio_num <- pi_prop * q_current
    ratio_num <- pi_prop * q_prop
    # ratio_den <- pi_current * q_prop
    ratio_den <- pi_current * q_current
    ratio <- ratio_num / ratio_den
    # ratio <- ratio_den / ratio_num
    
    u <- runif(n = 1, min = 0, max = 1)
    rho <- min(1, ratio)
    
    # Step 4.
    is_accepted <- 0
    if(u <= rho) {
      theta_current <- theta_prop
      is_accepted <- 1
    } else {
      # NULL
      # theta_current <- theta_current
    }
    mat_mcmc[i, ] <- 
      c(
        theta_current,
        theta_prop,
        q_current,
        q_prop,
        pi_current,
        pi_prop,
        ratio_num,
        ratio_den,
        ratio,
        rho, 
        u, 
        is_accepted
      )
  }
  res_mcmc <- 
    mat_mcmc %>% 
    as_tibble() %>% 
    mutate(idx = row_number()) %>% 
    select(idx, everything()) # %>% 
    # mutate_at(vars(is_accepted), ~ifelse(. == 1, TRUE, FALSE))
  
  if(!is.null(n_burnin)) {
    idx_slice <- (n_burnin + 1):nrow(mat_mcmc)
    res_mcmc <- res_mcmc %>% slice(idx_slice)
  }
  
  res_mcmc
}

do_mcmc_mh_q1 <- function(..., .n_mcmc = n_mcmc_q1, .n_burnin = n_burnin_q1) {
  .do_mcmc_mh_q1(n_mcmc = .n_mcmc, n_burnin = .n_burnin, ...)
}

```

```{r q1-a-5, include=T, echo=T, eval=T}
res_mcmc_mh_q1 <- do_mcmc_mh_q1()
res_mcmc_mh_q1
```

```{r q1-a-6, include=T, echo=T, eval=T}
y_mean <- y %>% mean()
y_mean
theta_mean <- 
  res_mcmc_mh_q1 %>% 
  summarise_at(vars(theta_current), mean) %>% 
  pull(1)
theta_mean 
```

```{r q1-a-6-hide, include=F, echo=F, eval=T}
y_mean_chr <- y_mean %>% format_num(3)
y_mean_chr
theta_mean_chr <- theta_mean %>% format_num(3)
theta_mean_chr
```


```{r q1-a-3-debug, include=F, echo=F, eval=F}
# res_nest <-
#   tibble(
#     theta_init = seq(-1.2, 1.2, length.out = 7)
#   ) %>% 
#   mutate(res = purrr::map(theta_init, ~do_mcmc_mh_q1(theta_init = .x)))
# res_nest
# res_nest %>% 
#   unnest() %>% 
#   filter(theta_current > -1.9) %>% 
#   ggplot() +
#   aes(x = theta_current) +
#   geom_histogram() +
#   scale_y_continuous(labels = scales::comma) +
#   facet_wrap(~theta_init) +
#   labs(
#     x = 'theta',
#     y = 'Frequency'
#   )
```


```{r q1-a-viz-debug-1, include=F, echo=F, eval=F}
# # res_mcmc_mh_q1 %>% filter(is_accepted)
# res_mcmc_mh_q1 %>% 
#   ggplot() +
#   aes(x = idx, y = theta_current) +
#   geom_point() +
#   geom_line()
# 
# res_mcmc_mh_q1 %>% 
#   sample_frac(0.1) %>% 
#   # select_if(is.numeric) %>% 
#   gather(key = 'key', value = 'value', -idx) %>% 
#   ggplot() +
#   aes(x = idx, y = value) +
#   geom_point() +
#   facet_wrap(~key, scales = 'free')
# 
# res_mcmc_mh_q1 %>% 
#   # select_if(is.numeric) %>% 
#   gather(key = 'key', value = 'value', -idx) %>% 
#   ggplot() +
#   aes(x = value) +
#   geom_histogram() +
#   facet_wrap(~key, scales = 'free')
```

```{r q1-a-viz-1-prep, include=F, echo=F, eval=T}
path_viz_q1 <- 'viz_mcmc_mh_q1.png'
w_viz_q1 <- 8
h_viz_q1 <- 5
eval_viz_q1 <- !fs::file_exists(path_viz_q1)
```

```{r viz-funcs, include=T, echo=T, eval=T}
theme_custom <- function(...) {
  theme_light() +
    theme(
      legend.position = 'bottom',
      legend.title = element_blank(),
      axis.title.x = element_text(hjust = 1),
      axis.title.y = element_text(hjust = 1),
      ...
    )
}
```

```{r q1-a-viz-1, include=T, echo=T, eval=eval_viz_q1}
viz_mcmc_mh_q1 <-
  res_mcmc_mh_q1 %>% 
  ggplot() +
  aes(x = theta_current) +
  geom_histogram(binwidth = 0.05) +
  geom_vline(aes(xintercept = y_mean, color = 'Mean of y'), size = 2) +
  geom_vline(aes(xintercept = theta_post_mean, color = 'Mean of theta'), size = 2) +
  scale_color_manual(values = c('red', 'blue')) +
  guides(color = guide_legend(override.aes = list(size = 2))) +
  scale_y_continuous(labels = scales::comma, expand = c(0, 0, 0, 10)) +
  # coord_cartesian(expand = FALSE) +
  theme_custom() +
  labs(
    x = 'theta',
    y = 'Frequency'
  )
```

```{r q1-a-viz-1-export, include=F, echo=F, eval=eval_viz_q1}
teproj::export_ext_png(
  viz_mcmc_mh_q1,
  path = path_viz_q1,
  units = 'in',
  width = w_viz_q1,
  height = h_viz_q1
)
```

```{r q1-a-viz-1-show-1, include=T, echo=T, eval=F}
viz_mcmc_mh_q1
```

```{r q1-a-viz-1-show-2, include=T, echo=F, eval=T}
knitr::include_graphics(path_viz_q1)
```
Note that the posterior mean $\hat{\theta}$ = `r theta_mean_chr` is close to the mean of the observations $\hat{y}$ = `r y_mean_chr`.



# 2. Gibbs Sampler and High/Low Protein Diet in Rats.

## Instructions

<instructions>
Armitage and Berry (1994, p. 111) report data on the weight gain of 19 female rats between 28 and 84 days after birth. The rats were placed in randomized manner on diets with high (12 animals) and low (7 animals) protein content.
</instructions>

| High protein | Low protein |
|--------------|-------------|
| 134          | 70          |
| 146          | 118         |
| 104          | 101         |
| 119          | 85          |
| 124          | 107         |
| 161          | 132         |
| 107          | 94          |
| 83           |             |
| 113          |             |
| 129          |             |
| 97           |             |
| 123          |             |

<instructions>
We want to test the hypothesis on dietary effect: Did a low protein diet result in a significantly lower weight gain? The classical $t$ test against the one sided alternative will be significant at 5% significance level, but we will not go in there. We will do the test Bayesian way using Gibbs sampler. Assume that high-protein diet measurements $y_{1j}$, $i = 1, \dots, 12$ are coming from normal distribution $\mathcal{N}(\theta_1, 1/\tau_1)$, where $\tau_1$ is the precision parameter,
</instructions>

$$
f(y_{1i} | \theta_1, \tau_1) \propto \tau_1^{1/2} \text{ exp } \{ -\frac{\tau_1}{2} (y_{1i} - \theta_1)^2 \}, \quad i = 1, \dots, 12.
$$

<instructions>
The low-protein diet measurements $y_{2i}$, $i = 1, \dots, 7$ are coming from normal distribution $\mathcal{N}(\theta_2, 1/\tau_2)$,
</instructions>

$$
f(y_{2i} | \theta_2, \tau_2) \propto \tau_2^{1/2} \text{ exp } \{ -\frac{\tau_2}{2} (y_{2i} - \theta_2)^2 \}, \quad i = 1, \dots, 7.
$$

<instructions>
Assume that $\theta_1$ and $\theta_2$ have normal priors $\mathcal{N}(\theta_{10}, 1/\tau_{10})$ and $\mathcal{N}(\theta_{20}, 1/\tau_{20})$, respectively. Take prior means as $\theta_{10} = \theta_{20} = 110$ (apriori no preference) and precisions as $\tau_{10} = \tau_{20} = 1/100$.<br/>
Assume that $\tau_1$ and $\tau_2$ have the gamma $\mathcal{Ga}(a_1, b_1)$ and $\mathcal{Ga}(a_2, b_2)$ priors with shapes $a_1 = a_2 - 0.01$ and rates $b_1 = b_2 = 4$.<br/>
(a) Construct Gibbs sampler that will sample $\theta_1, \tau_1, \theta_2$, and $\tau_2$ from their posteriors.<br/>
(b) Find sample differences $\theta_1 - \theta_2$. Proportion of positive differences approximates the posterior probability of hypothesis $H_0 : \theta_1 > \theta_2$. What is this proportion if the number of simulations is 10,000, with burn-in of 500?<br/>
(c) Using sample quantiles find the 95% equitailed credible set for $\theta_1 - \theta_2$. Does this set contain 0?<br/><br/>
Hint: No WinBUGS should be used (except maybe to check your results). Use Octave (MATLAB), or R, or Python here. You may want to consult Handout GIBBS.pdf from the course web repository.
</instructions>

## Response

### a

<response>
See the code and its output below.
</response>

```{r q2-ab-1, include=T, echo=T, eval=T}
# Data, constants, hyperparameters, and functions.
y_1 <- c(134, 146, 104, 119, 124, 161, 107, 107, 83, 113, 129, 97, 123)
y_2 <- c(70, 118, 101, 85, 107, 132, 94)

n_mcmc_q2 <- 10000
n_burnin_q2 <- 500

theta_1_0 <- 110
theta_2_0 <- theta_1_0
tau_1_0 <- 1 / 100
tau_2_0 <- tau_1_0
a_1_0 <- 0.01
a_2_0 <- a_1_0
b_1_0 <- 4
b_2_0 <- b_1_0

.compute_mu_new <- function(mu_i, tau_i, mu_0, tau_0, y_sum, n_obs) {
  mu_tau_0 <- mu_0 * tau_0
  mu_rnorm_num <- tau_i * y_sum + mu_tau_0
  mu_rnorm_den <- tau_0 + n_obs * tau_i
  mu_rnorm <- mu_rnorm_num / mu_rnorm_den
  sigma2_rnorm <- 1 / (tau_0 + n_obs * tau_i)
  sigma_rnorm <- sqrt(sigma2_rnorm)
  rnorm(1, mu_rnorm, sigma_rnorm)
}

y_sum_1 <- sum(y_1)
y_sum_2 <- sum(y_2)
n_obs_1 <- length(y_1)
n_obs_2 <- length(y_2)
compute_mu_new_1 <-
  function(mu_i,
           tau_i,
           .mu_0 = theta_1_0,
           .tau_0 = tau_1_0,
           .y_sum = y_sum_1,
           .n_obs = n_obs_1) {
    .compute_mu_new(
      mu_i,
      tau_i,
      mu_0 = .mu_0,
      tau_0 = .tau_0,
      y_sum = .y_sum,
      n_obs = .n_obs
    )
  }

compute_mu_new_2 <-
  function(mu_i,
           tau_i,
           .mu_0 = theta_2_0,
           .tau_0 = tau_2_0,
           .y_sum = y_sum_2,
           .n_obs = n_obs_2) {
    .compute_mu_new(
      mu_i,
      tau_i,
      mu_0 = .mu_0,
      tau_0 = .tau_0,
      y_sum = .y_sum,
      n_obs = .n_obs
    )
  }

.compute_tau_new <- function(mu_new, y, a_0, b_0, n_obs) {
  shape_rgamma <- a_0 + 0.5 * n_obs
  rate_rgamma  <- b_0 + 0.5 * sum((y - mu_new) ^ 2)
  rgamma(1, shape = shape_rgamma, rate = rate_rgamma) 
}

compute_tau_new_1 <-
  function(mu_new,
           .y = y_1,
           .a_0 = a_1_0,
           .b_0 = b_1_0,
           .n_obs = n_obs_1) {
    .compute_tau_new(
      mu_new = mu_new,
      y = .y,
      a_0 = .a_0,
      b_0 = .b_0,
      n_obs = .n_obs
    )
  }

compute_tau_new_2 <-
  function(mu_new,
           .y = y_2,
           .a_0 = a_2_0,
           .b_0 = b_2_0,
           .n_obs = n_obs_2) {
    .compute_tau_new(
      mu_new,
      y = .y,
      a_0 = .a_0,
      b_0 = .b_0,
      n_obs = .n_obs
    )
  }

theta_1_init <- mean(y_1) # theta_1_0
theta_2_init <- mean(y_2) # theta_1_i
tau_1_init <- 1 / sd(y_1) # tau_1_0
tau_2_init <- 1 / sd(y_2) # tau_1_i
```

```{r q2-ab-2, include=T, echo=T, eval=T}
# Main function.
.do_mcmc_gibbs_q2 <-
  function(...,
           n_mcmc = 10000,
           n_burnin = 1000,
           theta_1_init = 0,
           theta_2_init = 0,
           tau_1_init = 1,
           tau_2_init = 1) {
    stopifnot(is_likeinteger(n_mcmc))
    if (!is.null(n_burnin)) {
      stopifnot(is_likeinteger(n_burnin))
      stopifnot(n_burnin < n_mcmc)
    }
    stopifnot(is.numeric(theta_1_init))
    stopifnot(is.numeric(theta_2_init))
    stopifnot(is.numeric(tau_1_init))
    stopifnot(is.numeric(tau_2_init))
    cols_mat_mcmc <- c('theta_1', 'theta_2', 'tau_1', 'tau_2')
    mat_mcmc <- matrix(nrow = n_mcmc, ncol = length(cols_mat_mcmc))
    
    colnames(mat_mcmc) <- cols_mat_mcmc
    theta_1_i <- theta_1_init
    theta_2_i <- theta_2_init
    tau_1_i <- tau_1_init
    tau_2_i <- tau_2_init
    y_1_sum <- sum(y_1)
    y_2_sum <- sum(y_2)
    n_1_obs <- length(y_1)
    n_2_obs <- length(y_2)
    for (i in 1:n_mcmc) {
      theta_1_new <-
        compute_mu_new_1(
          mu_i = theta_1_i,
          tau_i = tau_1_i
        )
      
      theta_2_new <-
        compute_mu_new_2(
          mu_i = theta_2_i,
          tau_i = tau_2_i
        )
      
      tau_1_new <-
        compute_tau_new_1(
          mu_new = theta_1_new
        )
      
      tau_2_new <-
        compute_tau_new_2(
          mu_new = theta_2_new
        )
      mat_mcmc[i,] <-
        c(theta_1_new, theta_2_new, tau_1_new, tau_2_new)
      
      theta_1_i <- theta_1_new
      tau_1_i <- tau_1_new
      theta_2_i <- theta_2_new
      tau_2_i <- tau_2_new
      
    }
    res_mcmc <-
      mat_mcmc %>% 
      as_tibble() %>% 
      mutate(idx = row_number()) %>% 
      select(idx, everything())
    
    if (!is.null(n_burnin)) {
      idx_slice <- (n_burnin + 1):nrow(mat_mcmc)
      res_mcmc <- res_mcmc %>% slice(idx_slice)
    }
    
    res_mcmc
  }

do_mcmc_gibbs_q2 <-
  function(...,
           .n_mcmc = n_mcmc_q2,
           .n_burnin = n_burnin_q2,
           .theta_1_init = theta_1_init,
           .theta_2_init = theta_2_init,
           .tau_1_init = tau_1_init,
           .tau_2_init = tau_2_init) {
    .do_mcmc_gibbs_q2(
      n_mcmc = .n_mcmc,
      n_burnin = .n_burnin,
      theta_1_iniit = .theta_1_init,
      theta_2_init = .theta_2_init,
      tau_1_init = .tau_1_init,
      tau_2_init = .tau_2_init,
      ...
    )
  }
```

```{r q2-ab-3, include=T, echo=T, eval=T}
res_mcmc_gibbs_q2 <- do_mcmc_gibbs_q2()
res_mcmc_gibbs_q2
```

```{r q2-ab-4, include=F, echo=F, eval=F}
res_mcmc_gibbs_q2 %>% summarise_all(mean)
```

```{r q2-ab-viz-2-prep, include=F, echo=F, eval=T}
path_viz_q2 <- 'viz_mcmc_gibbs_q2.png'
w_viz_q2 <- w_viz_q1
h_viz_q2 <- h_viz_q1
eval_viz_q2 <- !fs::file_exists(path_viz_q2)
```

```{r q2-ab-viz-1, include=T, echo=T, eval=eval_viz_q2}
viz_mcmc_gibbs_q2 <-
  res_mcmc_gibbs_q2 %>% 
  gather(key = 'key', value = 'value', -idx) %>% 
  ggplot() +
  aes(x = value) +
  geom_histogram() +
  scale_y_continuous(labels = scales::comma) +
  theme_custom() +
  facet_wrap(~key, scales = 'free') +
  labs(
    x = NULL,
    y = 'Frequency'
  )
```

```{r q2-ab-viz-1-export, include=F, echo=F, eval=eval_viz_q2}
teproj::export_ext_png(
  viz_mcmc_gibbs_q2,
  path = path_viz_q2,
  units = 'in',
  width = w_viz_q2,
  height = h_viz_q2
)
```

```{r q2-ab-viz-1-show-1, include=T, echo=T, eval=F}
viz_mcmc_gibbs_q2
```

```{r q2-ab-viz-1-show-2, include=T, echo=F, eval=T}
knitr::include_graphics(path_viz_q2)
```

```{r q2-ab-5, include=F, echo=F, eval=T}
mcmc_gibbs_q2 <-
  res_mcmc_gibbs_q2 %>% 
  summarise_at(vars(-idx), list(sum = ~sum(.), mean = ~mean(.))) %>% 
  gather() %>% 
  deframe()
mcmc_gibbs_q2

theta_1_mean <- mcmc_gibbs_q2[['theta_1_mean']]
theta_1_mean
theta_1_mean_chr <- format_num(theta_1_mean, digits = 1)
theta_1_mean_chr
theta_2_mean <- mcmc_gibbs_q2[['theta_2_mean']]
theta_2_mean
theta_2_mean_chr <- format_num(theta_2_mean, digits = 1)
theta_2_mean_chr

y_1_mean <- mean(y_1)
y_1_mean
y_1_mean_chr <- format_num(y_1_mean, digits = 0)
y_1_mean_chr
y_2_mean <- mean(y_2)
y_2_mean
y_2_mean_chr <- format_num(y_2_mean, digits = 0)
y_2_mean_chr
```

As a quick check of the validity of our results, we can compare the posterior means of $\theta_1$ and $\theta_2$ (from the Gibbs sampling results) with those of the observations $y_1$ and $y_2$ and the priors $\theta_{10} = \theta_{20} = 110$. We find that $\hat{\theta_1}$ = `r theta_1_mean_chr`, which is between the observed mean $\hat{y_1}$ = `r y_1_mean_chr` and the prior 110; and we find that $\hat{\theta_2}$ = `r theta_2_mean_chr`, which is between the observed mean $\hat{y_2}$ = `r y_2_mean_chr` and 110. This is what we should have expected.

### b

We can calculate the proportion as follows.

```{r q2-b-1, include=T, echo=T, eval=T}
res_mcmc_gibbs_q2_calc <-
  res_mcmc_gibbs_q2 %>%
  mutate(
    theta_diff = theta_1 - theta_2
  ) %>% 
  mutate(
    h_0 = ifelse(theta_diff > 0, 1, 0)
  )
```

```{r q2-b-2, include=T, echo=T, eval=T}
h_0_q2 <-
  res_mcmc_gibbs_q2_calc %>% 
  summarise(h_0 = sum(h_0), n = n()) %>% 
  mutate(frac = h_0 / n)
h_0_q2
```

```{r q2-b-2-hide, include=F, echo=F, eval=T}
h_0_q2_frac <- h_0_q2 %>% pull(frac)
h_0_q2_frac
h_0_q2_frac_chr <- h_0_q2_frac %>% format_num(3)
h_0_q2_frac_chr
h_0_q2_pct_chr <- (h_0_q2_frac * 100) %>% format_num(1)
h_0_q2_pct_chr
```

<response>
We find that the proportion of positive differences is `r h_0_q2_frac_chr` (i.e. `r h_0_q2_pct_chr`%).
</response>


### c

We can calculate the credible set as follows.

```{r q2-c-1, include=T, echo=T, eval=T}
equi_credible_set_q2 <-
  res_mcmc_gibbs_q2_calc %>% 
  summarise(
    mu = mean(theta_diff), 
    sd = sd(theta_diff)
  ) %>% 
  mutate(
    l = qnorm(0.025, mu, sd),
    u = qnorm(0.975, mu, sd)
  )
equi_credible_set_q2
```

```{r q2-c-2, include=F, echo=F, eval=T}
credible_set_q2 <- equi_credible_set_q2 %>% select(l, u) %>% c()
credible_set_q2
```

```{r q2-c-3, include=F, echo=F, eval=T}
credible_set_chr_q2 <- credible_set_q2 %>% format_num(2)
credible_set_chr_q2
```

<response>
We find that the 95% equitailed credible set is [`r credible_set_chr_q2[1]`, `r credible_set_chr_q2[2]`]. Indeed, this set does include 0.
</response>


