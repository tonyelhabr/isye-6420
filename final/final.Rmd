---
title: 'ISYE 6420: Final'
author: 'aelhabr3'
output:
  html_document:
    css: ../hws/styles_hw.css
    theme: cosmo
    highlight: haddock
    toc: false
---

```{r setup, include=F, cache=F}
knitr::opts_knit$set(root.dir = here::here())
knitr::opts_chunk$set(
  # rows.print = 25,
  # rows.print = 25,
  echo = TRUE,
  cache = TRUE,
  # cache.lazy = FALSE,
  include = TRUE,
  fig.show = 'asis',
  fig.align = 'center',
  fig.width = 8,
  # size = 'small',
  # fig.height = 5,
  # fig.width = 5,
  # out.width = 5,
  fig.asp = 0.75,
  warning = FALSE,
  message = FALSE
)
options(width = 300)
```

```{r postprocess, include=F, echo=F, cache=F}
.path_sans_ext <- file.path('final')
.path_rmd <- paste0(.path_sans_ext, '.Rmd')
# spelling::spell_check_files(.path_rmd)
```

```{r setup-1, include=F, echo=F, eval=T}
library(tidyverse)
format_num <- function(x, digits = 4) {
  fmt <- sprintf('%%.%df', digits)
  sprintf(fmt, x)
}
```

# 1. Time to Second Birth.

## Instructions

<instructions>
...
</instructions>


## Response


We fit the following linear regression model.

$$
\begin{array}{c}
\text{ time } = \beta_0 + \beta_1 \text{ death } + \beta_2 \text{ mage }.
\end{array}
$$

The model code below is written so as to answer all parts of the question. The full model can be found in "q1.odc". (Note that we need to do anything "special" for the indicator variable `death`, i.e. transform its values to a 1-2 binary pair. It was found that the results are consistent either way.)

```{r setup-2}
library(tidyverse)
```

```{r data_list_q1, include=F, echo=F}
data_q1 <- 
  'q1-data.xlsx' %>% 
  readxl::read_excel()

n_q1 <- data_q1 %>% nrow()
data_list_q1 <-
  c(list(n = n_q1), as.list(data_q1))
```

```{r data_list_q1-datapasta, echo=F, include=F, eval=F}
# data_list_q1$time %>% datapasta::vector_paste()
# data_list_q1$mage %>% datapasta::vector_paste()
# data_list_q1$death %>% datapasta::vector_paste()
```

Below is the model code. The data is excerpted for the sake of readability. The full model can be found in "q1.odc".

```
model{
  for(i in 1:n) {
    mu[i] <- b0 + b1 * mage[i] + b2 * death[i]
    time[i] ~ dnorm(mu[i], tau)
  }

  # priors
  b0 ~ dnorm(0, 0.001)
  b1 ~ dnorm(0, 0.001)
  b2 ~ dnorm(0, 0.001)
  tau ~ dgamma(0.001, 0.001)
  sigma <- 1 / sqrt(tau)

  mu1 <- b0 + b1 * (24) + b2 * (0)
  pred1 ~ dnorm(mu1, tau)

  mu2 <- b0 + b1 * (28) + b2 * (1)
  pred2 ~ dnorm(mu2, tau)
}

# inits
list(b0 = -1, b1 = -1, b2 = 0, tau = 1, pred1 = 0, pred2 = 0)


# data
...
```

```{r res_sim_output_manual_q1, include=F, echo=F}
path_res_sim_output_manual_q1 <- 'q1-output.xlsx'
res_sim_output_manual_q1 <- 
  path_res_sim_output_manual_q1 %>% 
  readxl::read_excel()
res_sim_output_manual_q1
res_sim_output_q1 <- res_sim_output_manual_q1
```

Below is a summary of the output.

![](q1-output.png)

```{r res_sim_output_q1, echo=F, include=F, eval=F}
res_sim_output_q1
```

```{r pull_b_est_q1, include=F, echo=F}
.pull_est <- function(data, .var) {
  data %>% filter(var == .var) %>% pull(mean)
}
.pull_est_q1 <- purrr::partial(.pull_est, data = res_sim_output_q1, ... = )
.pull_b_est <- function(data, idx) {
  data %>% filter(var == sprintf('b%d', idx)) %>% pull(mean)
}
.pull_b_est_q1 <- purrr::partial(.pull_b_est, data = res_sim_output_q1, ... = )
```

We see that the estimated model is as follows.

```{r b_ests_q1, include=F, echo=F}
b_0_q1 <- .pull_b_est_q1(0)
b_1_q1 <- .pull_b_est_q1(1)
b_2_q1 <- .pull_b_est_q1(2)
```

$$
\begin{array}{c}
\text{ time } = (`r b_0_q1`) + (`r b_1_q1`) \text{ death } + (`r b_2_q1`) \text {mage }.
\end{array}
$$


### a

```{r b_2_q1, include=F, echo=F}
.var <- 'b2'
# Note: Use `b_2_res` since `b_2` is already used instead of `est_b_2`. (Unfortunately, naming convention is not completely consistent throughout this document.)
b_2_res_q1 <- res_sim_output_q1 %>% filter(var == .var)
b_2_res_q1
cs_b_2_q1 <- c(b_2_res_q1$val2.5pc, b_2_res_q1$val97.5pc)
cs_b_2_q1
```

<response>
From the output shown above, we see that the slope (i.e. the posterior estimate of the mean) of $\beta_2$ (for $\text{death}$) is `r b_2_q1`. Its 95% CS is [`r cs_b_2_q1[1]`, `r cs_b_2_q1[2]`]. This variable is significant (in this case, significantly negative) because its 95% CS does not contain zero. 
</response> (Yes, this is not exactly the most robust manner in which to determine significance, but it is usually a very good heuristic.)

### b

```{r b_1_q1, include=F, echo=F}
.var <- 'b1'
b_1_res_q1 <- res_sim_output_q1 %>% filter(var == .var)
b_1_res_q1
cs_b_1_q1 <- c(b_1_res_q1$val2.5pc, b_1_res_q1$val97.5pc)
cs_b_1_q1
```

<response>
We observe that the posterior estimate of the mean of $\beta_1$ (for $\text{mage}$) is `r b_1_q1`, and that its 95% CS is [`r cs_b_1_q1[1]`, `r cs_b_1_q1[2]`]. We may say that this variable is not significant (with respect to its relationship with $\text{time}$) because its 95% CS is not completely negative.<hide>Even though its posterior mean is negative, less than 95% of simulated estimates are negative---this is one way of interpreting a 95% CS---which fails our traditional criteria for ascertaining significance.</hide>
</response> 


### c

```{r pred1_q1, include=F, echo=F}
.var <- 'pred1'
pred1_q1 <- res_sim_output_q1 %>% filter(var == .var)
cs_pred1_q1 <- c(pred1_q1$val2.5pc, pred1_q1$val97.5pc)
cs_pred1_q1
est_pred1_q1 <- .pull_est_q1(.var)
est_pred1_q1
```

<response>
From the output shown above, we see that the predicted time between births for Helga (corresponding to `pred1`) is `r est_pred1_q1` with a 95% CS of [`r cs_pred1_q1[1]`, `r cs_pred1_q1[2]`].
</response> Notably, this CS is very wide, but it does not include zero, so we can be somewhat confident in it (and be mindful of its imprecision).

### d

```{r pred2_q1, include=F, echo=F}
.var <- 'pred2'
pred2_q1 <- res_sim_output_q1 %>% filter(var == .var)
cs_pred2_q1 <- c(pred2_q1$val2.5pc, pred2_q1$val97.5pc)
cs_pred2_q1
est_pred2_q1 <- .pull_est_q1(.var)
est_pred2_q1
```

<response>
From the output shown above, we see that the predicted time between births for Emma (corresponding to `pred2`) is `r est_pred2_q1` with 95% CS is [`r cs_pred2_q1[1]`, `r cs_pred2_q1[2]`].
</response> As with the prediction for Helga, this prediction's CS is very wide. It even includes zero (which does not make sense in the context of this problem)! Thus, we should be wary of the lack of robustness of this prediction.

# 2. Tasmanian Clouds.

## Instructions

<instructions>
...
</instructions>


## Response

We implement a comprehensive Bayesian additive two-way ANOVA analysis on the response `diff` to estimate and test the effects of factors `season` and `seeded`.

```{r data_list_q2, include=F, echo=F}
data_raw_q2 <- 
  'clouds.xlsx' %>% 
  readxl::read_excel()

data_q2 <-
  data_raw_q2 %>% 
  rename_all(tolower) %>% 
  mutate_at(
    vars(seeded),
    ~ifelse(. == 'U', 1L, 2L)
  ) %>% 
  mutate_at(
    vars(season),
    ~case_when(
      . == 'Spring' ~ 1L,
      . == 'Summer' ~ 2L,
      . == 'Autumn' ~ 3L,
      . == 'Winter' ~ 4L,
    )
  ) %>% 
  select(seeded, season, diff)

n_q2 <- data_q2 %>% nrow()
data_list_q2 <-
  c(list(n = n_q2), as.list(data_q2))
```

```{r data_list_q2-datapasta, echo=F, include=F, eval=F}
# data_list_q2$seeded %>% as.numeric() %>% datapasta::vector_paste()
# data_list_q2$season %>% as.numeric() %>% datapasta::vector_paste()
# data_list_q2$diff %>% round(3) %>% datapasta::vector_paste()
```

### a

Below is the model code, not including any interaction term between `seeded` and `season`. The full model can be found in "q2a.odc".

```
model{
  for(i in 1:n) {
    diff[i] ~ dnorm(mu[i], tau)
    mu[i] <- mu0 + alpha[seeded[i]] + beta[season[i]]
  }

  # STZ (sum-to-zero) constraints
  alpha[1] <- -sum(alpha[2:za])
  beta[1] <- -sum(beta[2:zb])

  # priors
  mu0 ~ dnorm(0, 0.0001)
  for(i in 2:za) {
    alpha[i] ~ dnorm(0, 0.0001)
  }
  for(i in 2:zb) {
    beta[i] ~ dnorm(0, 0.0001)
  }
  tau ~ dgamma(0.001, 0.001)
  sigma <- 1 / sqrt(tau)

  # pairwise comparisons
  for(i in 1:(za-1)) {
    for(j in (i+1):za) {
      ca[i, j] <- alpha[i] - alpha[j]
    }
  }
  for(i in 1:(zb-1)) {
    for(j in (i+1):zb) {
      cb[i, j] <- beta[i] - beta[j]
    }
  }
}

# inits
list(
  mu0 = 0,
  alpha = c(NA, 0),
  beta = c(NA, 0, 0, 0),
  tau = 1
)

# data
...
```

Note the following about the model.

+ It is very similar to that of the "simvastatin.odc" example.

+ The "U" and "S" for `seeded` have been re-coded as 1 and 2, and "Spring", "Summer", "Autumn", and "Winter" for `season` have been re-coded as 1, 2, 3, and 4, respectively.

```{r res_sim_output_manual_q2a, include=F, echo=F}
path_res_sim_output_manual_q2a <- 'q2a-output.xlsx'
res_sim_output_manual_q2a <- 
  path_res_sim_output_manual_q2a %>% 
  readxl::read_excel()
res_sim_output_manual_q2a
res_sim_output_q2a <- res_sim_output_manual_q2a
```

Below is a summary of the output.

![](q2a-output.png)

```{r res_sim_output_q2a, echo=F, include=F, eval=F}
res_sim_output_q2a
```

In the output above we are primarily interested in the comparison terms `ca` and `cb`. The `ca` terms quantify the difference in paired `seeded` values---either seeded "S" or unseeded "U". Here, there are only two possible values for `seeded`, so there is only one `ca` term, i.e. `ca[1, 2]`. (Really, an array data structure is not needed for the comparison of `seeded` terms, but it is used anyways to maintain consistency with the `season` comparison terms, of which there are more than one.) The `cb` terms quantify the difference between pairs of `season` values. For example, `cb[1, 2]` quantifies the difference between "Spring" and "Summer"; and `cb[1, 3]` quantifies the difference between "Spring" and "Autumn"; etc.

We consider those comparison terms with a 95% CS that is either completely positive or completely negative to be significant. (If we see that `val2.5pc > 0`, then we say that the comparison is significantly different in a positive way, favoring the first term; similarly, If we see that `val97.5pc < 0`, then we say that the comparison is significantly different in a negative way, favoring the second term.)

```{r viz-funcs, include=F, echo=F}
theme_custom <- function(...) {
  theme_light(base_size = 14) +
    theme(
      legend.position = 'bottom',
      # legend.title = element_blank(),
      axis.title.x = element_text(hjust = 1),
      axis.title.y = element_text(hjust = 1),
      ...
    )
}

export_png <-
  function(x,
           path,
           ...,
           units = 'in',
           width = 8,
           height = 5) {
    ggsave(
      plot = x,
      filename = path,
      units = units,
      width = width,
      height = height,
      ...
    )
  }


# .chr_sig <- 'Significant'
# .chrs_binary <- c('Positive', 'Negative')
# .lvls_boxplot <- 
#   c(
#     paste0(.chr_sig, .chrs_binary),
#     paste0('Not ', .chr_sig, .chrs_binary)
#   )
.lvls_boxplot <- 
  c(
    'Significant',
    'Not Significant'
  )

visualize_cs_c <- function(data, ...) {
  data %>% 
    filter(var %>% str_detect('^c')) %>% 
    mutate(
      descr = case_when(
        val2.5pc > 0 ~ .lvls_boxplot[1],
        val97.5pc < 0 ~ .lvls_boxplot[1],
        mean > 0 ~ .lvls_boxplot[2],
        # mean <= 0 ~ .lvls_boxplot[4]
        mean <= 0 ~ .lvls_boxplot[2]
      )
    ) %>% 
    mutate_at(vars(descr), ~ordered(., .lvls_boxplot)) %>% 
    ggplot() +
    aes(x = var, y = mean, color = descr) +
    geom_crossbar(
      aes(ymin = val2.5pc, ymax = val97.5pc),
      # width = 10,
      fill = 'grey90',
      fatten = 5
    ) +
    # scale_color_brewer(palette = 'RdYlGn') +
    # scale_x_reverse() +
    scale_color_manual(values = c('blue', 'grey50')) +
    guides(color = guide_legend(override.aes = list(size = 3))) +
    geom_hline(aes(yintercept = 0), size = 2) +
    theme_custom() +
    theme(
      legend.title = element_blank(),
      panel.grid.major.y = element_blank(),
      panel.grid.minor.y = element_blank()
    ) +
    coord_flip() +
    labs(
      x = NULL, y = NULL
    )
}

visualize_cs_ab <- function(data, ...) {
  data %>% 
    filter(var %>% str_detect('^(alpha|beta|mu0)')) %>% 
    mutate(
      descr = case_when(
        val2.5pc > 0 ~ .lvls_boxplot[1],
        val97.5pc < 0 ~ .lvls_boxplot[1],
        mean > 0 ~ .lvls_boxplot[2],
        # mean <= 0 ~ .lvls_boxplot[4]
        mean <= 0 ~ .lvls_boxplot[2]
      )
    ) %>% 
    mutate_at(vars(descr), ~ordered(., .lvls_boxplot)) %>% 
    ggplot() +
    aes(x = var, y = mean, color = descr) +
    geom_pointrange(
      aes(ymin = val2.5pc, ymax = val97.5pc),
      # fatten = 5
      size = 2,
      fill = 'grey90'
    ) +
    # scale_color_brewer(palette = 'RdYlGn') +
    # scale_x_reverse() +
    scale_color_manual(values = c('blue', 'grey50')) +
    guides(color = guide_legend(override.aes = list(size = 3))) +
    geom_hline(aes(yintercept = 0), size = 2) +
    theme_custom() +
    theme(
      legend.title = element_blank(),
      panel.grid.major.y = element_blank(),
      panel.grid.minor.y = element_blank()
    ) +
    coord_flip() +
    labs(
      x = NULL, y = NULL
    )
}
```

To assist with interpreting the comparison terms, below is a plot illustrating the posterior means and 95% CS of the two-way paired comparisons. (The interior crossbars illustrate the mean and the box bounds illustrate the 95% CS range.)

```{r path_viz_q2a_1, include=F, echo=F}
path_viz_q2a_1 <- 'viz_q2a_1.png'
eval_viz_q2a_1 <- !fs::file_exists(path_viz_q2a_1)
```

```{r viz_q2a_1, echo=F, include=F, eval=T, fig.show=F}
.lab_title_q2a_1 <- 'Comparison Terms, Two-Way ANOVA Without Interaction Term'
viz_q2a_1 <-
  res_sim_output_q2a %>% 
  visualize_cs_c() +
  labs(
    subtitle = .lab_title_q2a_1
  )
viz_q2a_1
```

```{r viz_q2a_1-export, include=F, echo=F, eval=eval_viz_q2a_1}
export_png(
  viz_q2a_1,
  path = path_viz_q2a_1,
)
```

```{r viz_q2a-show, include=T, echo=F}
knitr::include_graphics(path_viz_q2a_1)
```
<response>
We observe the following:
</response>

+ <response>The three `season` comparison terms involving "Winter" are significantly positive: (1) `cb[1,4]` (i.e. the difference between "Spring" and "Winter"); (2) `cb[2 4]` (i.e. the difference between "Summer" and "Winter"); (3) `cb[3,4]` (i.e. the difference between "Autumn" and "Winter").</response>

+ <response>Two terms have negative posterior means but are not significant: `ca[1,2]` and `cb[1,2]`.</response>
 
+ <response>The remaining two terms---involving `season` comparisons with "Autumn"---have positive posterior means but are not significant: `cb[1,3]` and `cb[2,3]`.</response>

Of course, we should also look at the estimates of the `alpha` and `beta` terms, corresponding to `seeded` and `season` respectively.

```{r path_viz_q2a_2, include=F, echo=F}
path_viz_q2a_2 <- 'viz_q2a_2.png'
eval_viz_q2a_2 <- !fs::file_exists(path_viz_q2a_2)
```

```{r viz_q2a_2, echo=F, include=F, eval=T, fig.show=F}
.lab_title_q2a_2 <- .lab_title_q2a_1 %>% str_replace('Comparison Terms, ', 'Coefficient Terms, ')
viz_q2a_2 <-
  res_sim_output_q2a %>% 
  visualize_cs_ab() +
  labs(
    subtitle = .lab_title_q2a_2
  )
viz_q2a_2
```

```{r viz_q2a_2-export, include=F, echo=F, eval=eval_viz_q2a_2}
export_png(
  viz_q2a_2,
  path = path_viz_q2a_2,
)
```

```{r viz_q2a_2-show, include=T, echo=F}
knitr::include_graphics(path_viz_q2a_2)
```

<response>
We observe the following.
</response>

+ <response>The `alpha` terms are insignificant because the CS includes zero. Also, note that `alpha[1]` and `alpha[2]`---representing "U" (unseeded) and "S" (seeded) respectively---are essentially inverses of one another.</response>

+ <response>Three `beta` terms are significant. `beta[1]` and `beta[1]`---representing "Spring" and "Summer" respectively---are significantly positive; `beta[4]`--- representing "Winter---is significantly negative.</response>

+ <response>`beta[3]`---representing "Autumn"---is close to zero and is insignificant. Still, it is significantly different from `beta[4]`, as indicated before with `cb[3,4]`. (Note that its CS does not overlap with that of `beta[4]`.)</response>

+ <response>The intercept term `mu0` is nearly zero and is insignificant.</response>

### b

Below is the code for the model including the interaction term (i.e. `alpha.beta`). The full model can be found in "q2b.odc".

```
model{
  for(i in 1:n) {
    diff[i] ~ dnorm(mu[i], tau)
    mu[i] <- mu0 + alpha[seeded[i]] + beta[season[i]] + alpha.beta[seeded[i], season[i]]
  }

  # STZ (sum-to-zero) constraints
  alpha[1] <- -sum(alpha[2:za])
  beta[1] <- -sum(beta[2:zb])
  for(i in 1:za) {
    alpha.beta[i, 1] <- -sum(alpha.beta[i, 2:zb])
  }
  for(i in 2:zb) {
    alpha.beta[1, i] <- -sum(alpha.beta[2:za, i])
  }

  # priors
  mu0 ~ dnorm(0, 0.0001)
  for(i in 2:za) {
    alpha[i] ~ dnorm(0, 0.0001)
  }
  for(i in 2:zb) {
    beta[i] ~ dnorm(0, 0.0001)
  }
  for(i in 2:za) {
    for(j in 2:zb) {
      alpha.beta[i, j] ~ dnorm(0, 0.0001)
    }
  }
  tau ~ dgamma(0.001, 0.001)
  sigma <- 1 / sqrt(tau)

  # pairwise comparisons
  for(i in 1:(za-1)) {
    for(j in (i+1):za) {
      ca[i, j] <- alpha[i] - alpha[j]
    }
  }
  for(i in 1:(zb-1)) {
    for(j in (i+1):zb) {
      cb[i, j] <- beta[i] - beta[j]
    }
  }
}

# inits
list(
  mu0 = 0,
  alpha = c(NA, 0),
  beta = c(NA, 0, 0, 0),
  alpha.beta = structure(.Data = c(NA, NA, NA, NA, NA, 0, 0, 0), .Dim = c(2, 4)),
  tau = 1
)

# data
...
```

Note that this code is very similar to that from (a) above, with additions made to include the interaction term `alpha.beta`.


```{r res_sim_output_manual_q2b, include=F, echo=F}
path_res_sim_output_manual_q2b <- 'q2b-output.xlsx'
res_sim_output_manual_q2b <- 
  path_res_sim_output_manual_q2b %>% 
  readxl::read_excel()
res_sim_output_manual_q2b
res_sim_output_q2b <- res_sim_output_manual_q2b
```

Below is a summary of the output.

![](q2b-output.png)

```{r res_sim_output_q2b, echo=F, include=F, eval=F}
res_sim_output_q2b
```

<response>
We observe the that all interaction terms are insignificant. (Each interaction term's CS includes zero.) Also, we see that the posterior means and 95% CS of all the `seeded` and `season` terms are only trivially affected. (There is no use in re-creating the same plots shown in (a).)
</response>

# 3. Miller Lumber Company Customer Survey.

## Instructions

<instructions>
...
</instructions>


## Response

We fit the following Poisson regression model.

$$
\begin{array}{rcl}
\log(\text{customers}) & \sim & \beta_0 + \beta_1 \text{ hunits } + \beta_2 \text{ aveinc } + \\
& & \beta_3 \text{ aveage } + \beta_4 \text{ distcomp } + \beta_5 \text{ diststore }
\end{array}
$$

The model code below is written so as to answer parts (a) and (c). (In particular, `lambdastar` and `pred1` correspond to the mean response and prediction asked for in (c). The full model can be found in "q3ac.odc".

```{r data_list_q3, include=F, echo=F}
data_q3 <- 
  'q3-data.xlsx' %>% 
  readxl::read_excel()

n_q3 <- data_q3 %>% nrow()
data_list_q3 <-
  c(list(n = n_q3), as.list(data_q3))
data_list_q3
```

```{r data_list_q3_datapasta, include=F, echo=F}
# data_list_q3$customers %>% datapasta::vector_paste()
# data_list_q3$hunits %>% {./ 1000} %>% datapasta::vector_paste()
# data_list_q3$aveinc %>% {./ 10000} %>% datapasta::vector_paste()
# data_list_q3$avehage %>% datapasta::vector_paste()
# data_list_q3$diststore %>% datapasta::vector_paste()
# data_list_q3$distcomp %>% datapasta::vector_paste()
```

```
model {
  for(i in 1:n){
    customers[i] ~ dpois(lambda[i])
    log(lambda[i]) <- b0 + b1 * hunits[i] + b2 * aveinc[i] + b3 * avehage[i] + b4 * distcomp[i] + b5 * diststore[i]
  }

  # prediction
  log(lambdastar) <- b0 + b1 * (0.72) + b2 * (7) + b3 * (6) + b4 * (4.1) + b5 * (8)
  pred1 ~ dpois(lambdastar)

  # priors
  b0 ~ dnorm(0, 0.01)
  b1 ~ dnorm(0, 0.01)
  b2 ~ dnorm(0, 0.01)
  b3 ~ dnorm(0, 0.01)
  b4 ~ dnorm(0, 0.01)
  b5 ~ dnorm(0, 0.01)
}

# inits
list(b0 = 0, b1 = 0, b2 = 0, b3 = 0, b4 = 0, b5 = 0, pred1 = 0)


# data
# ...
```

Note the following about the model.

+  As advised by the announcement made on Canvas, the values of the variable $\text{hunits}$ (for home units) has been scaled down by a factor of 1,000 (i.e. divided by 1,000) and the values of the variable $\text{aveinc}$ (for salaries) has been scaled down by a factor of 10,000.

+ Following a second recommendation from the Canvas announcement, we make the $\beta$ terms not "too noninformative", defining them as `dnorm(0, 0.01)` instead of something like `dnorm(0, 0.000001)`.


```{r res_sim_output_manual_q3, include=F, echo=F}
path_res_sim_output_manual_q3 <- 'q3ac-output.xlsx'
res_sim_output_manual_q3 <- 
  path_res_sim_output_manual_q3 %>% 
  readxl::read_excel()
res_sim_output_manual_q3
res_sim_output_q3 <- res_sim_output_manual_q3
```

Below is a summary of the output.

![](q3ac-output.png)

```{r res_sim_output_q3, echo=F, include=F, eval=F}
res_sim_output_q3
```

```{r pull_b_est_q3, include=F, echo=F}
.pull_b_est_q3 <- purrr::partial(.pull_b_est, data = res_sim_output_q3, ... = )
```

### a

```{r b_ests_q3, include=F, echo=F}
b_1_q3 <- .pull_b_est_q3(1)
b_2_q3 <- .pull_b_est_q3(2)
b_3_q3 <- .pull_b_est_q3(3)
b_4_q3 <- .pull_b_est_q3(4)
b_5_q3 <- .pull_b_est_q3(5)
```

As shown in the output above, <response>the coefficient estimates (i.e. the $\beta$ posterior means) are as follows.</response>

+ $\beta_{1}$ = `r b_1_q3`

+ $\beta_{2}$ = `r b_2_q3`

+ $\beta_{3}$ = `r b_3_q3`

+ $\beta_{4}$ = `r b_4_q3`

+ $\beta_{5}$ = `r b_5_q3`

All terms seem to be significant, given that their 95% CS are either completely negative or completely positive.

### b

In order to identify the "best" two covariates from the set of five, we implement stochastic search variable selection (SSVS), as discussed in Unit 9 and exemplified in "Haldssvs" from the Unit 9 exercises. The theory behind this process is as follows (specified for Poisson regression instead of traditional linear regression).

$$
\begin{array}{rcl}
\log(\lambda) & = & \beta_0 + \beta_1 x_1 + \cdots + \beta_k x_k, \\
\beta_i & = & \delta \alpha_i, \\
\alpha_i & \sim & \mathcal{Norm}(0, \tau), \\
\delta_i & \sim & \mathcal{Bern}(p_i).
\end{array}
$$

where $p_i$ is the probability that the variable $x_i$ is in the model. Note that the indicator $\delta_i$ terms are used simply to determine whether a $\beta_i$ term is included in a given model. That is, if $\delta_i = 0$, then $\beta_i$ is not included; and if $\delta_i = 1$, then $\beta_i$ is included in the model and its estimate is that of $\alpha_i$. In the end, we choose the model with the greatest a posteriori probability.

```{r data_list_q3b, include=F, echo=F}
data_q3b <-
  data_q3 %>% 
  mutate_at(vars(hunits), ~{. / 1000} %>% round(3)) %>% 
  mutate_at(vars(aveinc), ~{. / 10000} %>% round(4)) %>% 
  set_names(c('y[]', sprintf('x[,%d]', 1:5)))
# write_tsv(data_q3b, 'q3b1-data.txt')
# Using `write.table()` so that "\r\n" is EOL character, which enables easy copy-paste into OpenBUGs.
export_tsv <- purrr::partial(write.table, quote = FALSE, sep = '\t', row.names = FALSE, ... = )
export_tsv(data_q3b, 'q3b1-data.txt')
data_q3b %>% 
  select(matches('y|[245]')) %>%
  set_names(c('y[]', sprintf('x[,%d]', 1:3))) %>% 
  export_tsv('q3b2-data.txt')
```

The (initial) model code is shown below. (This is actually a two-step process, given the ambiguity of the initial results.) The full model files are "q3b1.odc" and "q3b2.odc".

```
model {
  for(i in 1:n){
    y[i] ~ dpois(lambda[i])
    log(lambda[i]) <- int + inprod(x.c[i,], beta[])
  }
  int ~ dnorm(0,0.001)

  # SSVS prior
  for(j in 1:p){
    delta[j] ~ dbern(0.5)
    alpha[j] ~ dnorm(0, 0.1)
    beta[j] <- delta[j] * alpha[j]
  }

  # model probabilities
  for(j1 in 1:2){
    for(j2 in 1:2){
      for(j3 in 1:2){
        for(j4 in 1:2){
          for(j5 in 1:2){
            model[j1, j2, j3, j4, j5] <- equals(delta[1], j1 - 1) * equals(delta[2], j2 - 1) * equals(delta[3], j3 - 1) * equals(delta[4], j4 - 1) * equals(delta[5], j5 - 1)
          }
        }
      }
    }
  }

  # data centering
  for(i in 1:n){
    for(j in 1:p){
      x.c[i,j] <- (x[i, j] - mean(x[, j])) / sd(x[, j])
    }
  }
}

# inits
list(
  delta = c(1, 1, 1, 1, 1),
  alpha = c(0, 0, 0, 0, 0),
  int = 1
)

# data
...
```

Note the following about the model.

+ For the sake of convenience, we rename the features `hunits`, ..., `diststore` to be `x[,1]`, ..., `x[,5]` with corresponding $\beta$ estimates `b1`, ..., `b5`.

+  For our implementation, we define $\tau = 0.01$ (shown in the stochastic process equations above) so that the $\alpha_i$ estimates are relatively noninformative. (This maintains the premise that we should make our priors "not too noninformative".)

+ We center the data---creating the set of `x.c[]` variables. This is a standard pre-processing step in most variable selection procedures.

+ We use the values `2` and `1` instead of `1` and `0` as our binary pair for included and non-included terms. (This is convenient for OpenBUGs.)


```{r res_sim_output_manual_q3b1, include=F, echo=F}
path_res_sim_output_manual_q3b1 <- 'q3b1-output.xlsx'
res_sim_output_manual_q3b1 <- 
  path_res_sim_output_manual_q3b1 %>% 
  readxl::read_excel()
res_sim_output_manual_q3b1
res_sim_output_q3b1 <- res_sim_output_manual_q3b1
```

Below is a summary of the output.

![](q3b1-output.png)

```{r res_sim_output_q3b1, echo=F, include=F, eval=F}
res_sim_output_q3b1
```

```{r ex_q3b1, include=F, echo=F}
.pull_est_q3b1 <- purrr::partial(.pull_est, data = res_sim_output_q3b1, ... = )
delta_ex_q3b1 <- .pull_est_q3b1('delta[1]')
model_ex_q3b1 <- .pull_est_q3b1('model[2,2,1,2,2]')
```

There is quite a bit to look at here. To provide some guidance for interpreting these results, take the following examples. 

+ Note that `model[2, 2, 1, 2, 2]` has a posterior `mean` estimate of `r model_ex_q3b1`. This tells us that the model with `x[,1]` (for `hunits`, corresponding to the first `2`), with `x[,2]`(for `aveinc`, corresponding to the second `2`), without `x[,3]` (for `avehage`, corresponding the `1`), with `x[,4]` (for `distcomp`, corresponding to the third `2`) and with `x[,5]` (for `diststore`, corresponding to the fourth `2`) is visited `r scales::percent(model_ex_q3b1)` of the time by the MCMC procedure.

+ Note that `delta[1]` has a posterior `mean` estimate of `r delta_ex_q3b1`. This means that the `x1` term (for `hunits`) is selected `r scales::percent(delta_ex_q3b1)` of the time by the MCMC procedure.

Overall, we observe that there are many model combinations with posterior `mean` estimates (of probability) equal to 0. Since the only combinations that have nonzero probabilities include three or more terms, we must repeat the SSVS process. Since the `delta` terms corresponding to `x[,2]`, `x[,4]`, `x[,5]` equally have the highest posterior `mean`s, we reduce our data set to just these three terms.

Since the model code is nearly identical, it is not shown here. The only non-trivial change to note is that the terms `x[,2]`, `x[,4]`, `x[,5]` are "re-indexed" to `x[,1]`, `x[,2]`, `x[,3]`. Please consult the "q3b2.odc" file to see the implementation.


```{r res_sim_output_manual_q3b2, include=F, echo=F}
path_res_sim_output_manual_q3b2 <- 'q3b2-output.xlsx'
res_sim_output_manual_q3b2 <- 
  path_res_sim_output_manual_q3b2 %>% 
  readxl::read_excel()
res_sim_output_manual_q3b2
res_sim_output_q3b2 <- res_sim_output_manual_q3b2
```

Below are the results of this second SSVS procedure.


![](q3b2-output.png)

We have exactly one two-covariate model---`model[1,2,2]`---with a nonzero posterior `mean`, which makes our choice of two-covariate model straightforward. (Of course, if we were able to choose three variables, then we could have chosen all three.). <response>The terms included in this "best" model (according to SSVS) are `distcomp` and `diststore`.</response>

### c

The mean response for the new data (`hunits = 720`, `aveinc = 70000`, `aveage = 6`, `distcomp = 4.1`, and `diststore = 8`) correspond to `lambdastar` from the output shown above (a). Similarly, the prediction corresponds to `pred1`.

```{r pred1_mean_q3c, include=F, echo=F}
.pull_est_q3 <- purrr::partial(.pull_est, data = res_sim_output_q3, ... = )
.var <- 'lambdastar'
pred1_mean_q3c <- res_sim_output_q3 %>% filter(var == .var)
pred1_mean_q3c
cs_pred1_mean_q3c <- c(pred1_mean_q3c$val2.5pc, pred1_mean_q3c$val97.5pc)
cs_pred1_mean_q3c
est_pred1_mean_q3c <- .pull_est_q3(.var)
est_pred1_mean_q3c
```

```{r pred1_q3c, include=F, echo=F}
.var <- 'pred1'
pred1_q3c <- res_sim_output_q3 %>% filter(var == .var)
cs_pred1_q3c <- c(pred1_q3c$val2.5pc, pred1_q3c$val97.5pc)
cs_pred1_q3c
est_pred1_q3c <- .pull_est_q3(.var)
est_pred1_q3c
```

<response>
There we see that the mean response is `r est_pred1_mean_q3c` with a 95% CS of [`r cs_pred1_mean_q3c[1]`, `r cs_pred1_mean_q3c[2]`]. The prediction is `r est_pred1_q3c` with a 95% CS of [`r cs_pred1_q3c[1]`, `r cs_pred1_q3c[2]`].
</response> Note that the prediction has a larger CS than that of the mean response. This is because the prediction's CS accounts for additional prediction---the uncertainty of a point estimate prediction. (We explored this concept in question 2(c) of HW6.)

<hide>
See p. 187 at https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/bugsbook_chapter9.pdf for more about this explanation.
</hide>
