---
title: 'ISYE 6420: Midterm'
author: 'aelhabr3'
output:
  html_document:
    css: ../hws/styles_hw.css
    theme: cosmo
    highlight: haddock
    toc: false
---

```{r setup, include=F, cache=F}
knitr::opts_knit$set(root.dir = here::here())
knitr::opts_chunk$set(
  echo = TRUE,
  # cache = FALSE,
  cache = TRUE,
  include = TRUE,
  fig.show = 'asis',
  fig.align = 'center',
  fig.width = 8,
  # size = "small",
  # fig.height = 5,
  # fig.width = 5,
  # out.width = 5,
  fig.asp = 0.75,
  warning = FALSE,
  message = FALSE
)
```

```{r postprocess, include=F, echo=F, cache=F}
.path_sans_ext <- file.path('midterm')
.path_rmd <- paste0(.path_sans_ext, '.Rmd')
# spelling::spell_check_files(.path_rmd)
```

```{r setup-1, include=F, echo=F, eval=T}
format_num <- function(x, digits = 4) {
  fmt <- sprintf('%%.%df', digits)
  sprintf(fmt, x)
}
```

# 1. Emily, Car, Stock Market, Sweepstakes, Vacation and Bayes.

## Instructions

<instructions>
Emily is taking Bayesian Analysis course...
</instructions>

## Response

Below I give my response in terms of exact calculation, using `R` to perform calculations. In the notation and code that follows, I use `a`, `b`, `c` for Emily's `grade`; `bull` and `bear` to indicate bullish or bearish `market`; `yes` or `no` to indicate whether Emily's uncle buys here a `car` and whether she goes on `vaca`tion (to Redington Shores); and `win` or `lose` to indicate if she wins the `lott`ery (i.e. sweepstakes).

### a

We want to find $\Pr(\text{car=yes|vaca=yes})$. This can be calculated using Bayes' theorem as follows.

$$
\begin{aligned}
\Pr(\text{car=yes|vaca=yes})
& = \frac{\Pr(\text{vaca=yes|car=yes}) \Pr(\text{car=yes})}{\Pr(\text{vaca=yes})} \\
& = \frac{\Pr(\text{vaca=yes,car=yes})}{\Pr(\text{vaca=yes})}
\end{aligned}
$$

In the code that follows, I delineate all possible scenarios in a manner, calculating the joint probability for each. This is essentially like drawing a tree diagram or notating all scenarios such as follows.

$$
\begin{array}{l}
\text{Scenario A: lott=win,grade=a,market=bull,car=yes,vaca=yes} \\
\text{Scenario B: lott=win,grade=a,market=bear,car=yes,vaca=yes} \\
\dots,
\end{array}
$$

```{r setup-2}
library(tidyverse)
```

```{r q1-0}
p_a <- 0.6
p_b <- 0.3
p_c <- 0.1
p_bull <- 0.5
p_bear <- 0.5
p_car_given_a_bull <- 0.8
p_car_given_b_bull <- 0.5
p_car_given_c_bull <- 0.2
p_car_given_a_bear <- 0.5
p_car_given_b_bear <- 0.3
p_car_given_c_bear <- 0.1
p_vaca_given_car <- 0.7
p_vaca_given_no_car <- 0.2
p_lott <- 0.001
p_vaca_given_lott <- 0.99
```

```{r q1-a-1-pre, include=F, echo=F}
opt_old <- getOption('tibble.print_min')
options(tibble.print_min = Inf)
```

```{r q1-a-1}
states <-
  crossing(
    lott = c('win', 'lose'),
    grade = c('a', 'b', 'c'),
    market = c('bull', 'bear'),
    car = c('yes', 'no'),
    vaca = c('yes', 'no')
  ) %>% 
    mutate_at(
    vars(lott),
    list(p_lott = ~case_when(
      . == 'win' ~ p_lott,
      . == 'lose' ~ 1 - p_lott
    ))
  ) %>% 
  mutate_at(
    vars(grade),
    list(p_grade = ~case_when(
      . == 'a' ~ 0.6,
      . == 'b' ~ 0.3,
      . == 'c' ~ 0.1
    ))
  ) %>% 
  mutate_at(
    vars(market),
    list(p_market = ~case_when(
      . == 'bull' ~ 0.5,
      . == 'bear' ~ 0.5
    ))
  ) %>% 
  mutate_at(
    vars(market),
    list(p_market = ~0.5)
  ) %>% 
  mutate_at(
    vars(car),
    list(p_car = ~case_when(
      grade == 'a' & market == 'bull' & . == 'yes' ~ p_car_given_a_bull,
      grade == 'a' & market == 'bull' & . == 'no' ~ 1 - p_car_given_a_bull,
      grade == 'a' & market == 'bear' & . == 'yes' ~ p_car_given_a_bear,
      grade == 'a' & market == 'bear' & . == 'no' ~ 1 - p_car_given_a_bear,
      grade == 'b' & market == 'bull' & . == 'yes' ~ p_car_given_b_bull,
      grade == 'b' & market == 'bull' & . == 'no' ~ 1 - p_car_given_b_bull,
      grade == 'b' & market == 'bear' & . == 'yes' ~ p_car_given_b_bear,
      grade == 'b' & market == 'bear' & . == 'no' ~ 1 - p_car_given_b_bear,
      grade == 'c' & market == 'bull' & . == 'yes' ~ p_car_given_c_bull,
      grade == 'c' & market == 'bull' & . == 'no' ~ 1 - p_car_given_c_bull,
      grade == 'c' & market == 'bear' & . == 'yes' ~ p_car_given_c_bear,
      grade == 'c' & market == 'bear' & . == 'no' ~ 1 - p_car_given_c_bear
    ))
  ) %>% 
  mutate_at(
    vars(vaca),
    list(p_vaca = ~case_when(
      lott == 'win' & car == 'yes' & . == 'yes' ~ p_vaca_given_lott,
      lott == 'win' & car == 'yes' & . == 'no' ~ 1 - p_vaca_given_lott,
      lott == 'win' & car == 'no' & . == 'yes' ~ p_vaca_given_lott,
      lott == 'win' & car == 'no' & . == 'no' ~ 1 - p_vaca_given_lott,
      lott == 'lose' & car == 'yes' & . == 'yes' ~ p_vaca_given_car,
      lott == 'lose' & car == 'yes' & . == 'no' ~ 1 - p_vaca_given_car,
      lott == 'lose' & car == 'no' & . == 'yes' ~ p_vaca_given_no_car,
      lott == 'lose' & car == 'no' & . == 'no' ~ 1 - p_vaca_given_no_car
    ))
  ) %>% 
  mutate(
    p = p_lott * p_grade * p_market * p_car * p_vaca
  ) %>% 
  mutate_at(vars(lott), ~factor(., levels = c('lose', 'win'))) %>%
  mutate_at(vars(grade), ~factor(.)) %>% 
  mutate_at(vars(market), ~factor(., levels = c('bull', 'bear'))) %>% 
  mutate_at(vars(car, vaca), ~factor(., levels = c('yes', 'no'))) %>% 
  arrange(lott, grade, market, car, vaca)
states
```

```{r q1-a-1-post, include=F, echo=F}
options(tibble.print_min = opt_old)
```

The data.frame `states` describes all possible scenarios (24 total) and probabilities. (Note that the `p_*` columns describe the probabilities, and the `p` column is the product of these.) This data.frame is essentially equivalent to a probability tree diagram partitioning out the entire sample space.

In case the reader has any doubts about the probabilities in the data.frame, we can do a quick check of the code and show that the probabilities of all scenarios sum up to 1.

```{r q1-a-1-check}
states %>% summarise_at(vars(p), sum) %>% pull(p)
```

<i>
Also, before moving on, I should note that we can disregarding the case where Emily goes on vacation after winning the lottery because it is independent of the non-lottery scenarios. ("Independently, Emily
may be a lucky winner...".)
</i>

Now, to begin filling in numbers for our Bayes' equation above, let's begin by finding $\Pr(\text{vaca|yes})$. Filtering for just the `vaca == 'yes'` scenarios, we find that ther are `r nrow(dplyr::filter(states, vaca == 'yes'))` records.

```{r q1-a-1-expl}
states %>% filter(vaca == 'yes')
```

```{r q1-a-1-expl-debug, include=F, echo=F, eval=F}
states %>% 
  mutate(rn = row_number()) %>% 
  filter(rn %in% c(3,7,11,15,19,23,27,31,35,39,43,47)) #  vaca = no

states %>% 
  mutate(rn = row_number()) %>% 
  filter(rn %in% c(1,5,9,13,17,21,25,29,33,37,41,45)) # vaca  = yes
  
states %>% 
  mutate(rn = row_number()) %>% 
  filter(rn %in% seq(1,24,2))

states %>% 
  mutate(rn = row_number()) %>% 
  filter(rn %in% seq(25,48,2))
```

And we can view the marginal probabilities of the going on vacation or not by summing by group.

```{r q1-a-2-pre}
states_vaca <-
  states %>% 
  group_by(vaca) %>% 
  summarise_at(vars(p), sum) %>% 
  ungroup()
states_vaca
```

```{r q1-a-2}
p_vaca <- 
  states_vaca %>% 
  filter(vaca == 'yes') %>% 
  pull(p)
p_vaca
```

```{r q1-a-2-chr, include=F, echo=F}
p_vaca_chr <- format_num(p_vaca)
p_vaca_chr
```

Summing up the probabilities for the scenarios where Emily does go on vacation gives us $P(\text{vaca|yes})$ = `r p_vaca_chr`, which is the denominator of the Bayes' equation noted above.

Next, we can get the joint probabilities of Emily getting a car and going on vacation.

```{r q1-a-3}
p_car_and_vaca <-
  states %>% 
  filter(car == 'yes', vaca == 'yes') %>% 
  summarise_at(vars(p), sum) %>% 
  pull(p)
p_car_and_vaca
```

```{r q1-a-3-chr, include=F, echo=F}
p_car_and_vaca_chr <- format_num(p_car_and_vaca, 3)
p_car_and_vaca_chr
```

We see that the probability of $\Pr(\text{car=yes,vaca=yes})$ = `r p_car_and_vaca_chr`, which is a term we need for the numerator of our Bayes' equation.

Finally, since we already have $\Pr(\text{vaca=yes|car=yes})$ = `r p_vaca_given_car`, we can now "plug-and-chug" to calculate $\Pr(\text{car=yes|vaca=yes})$.

```{r q1-a-4}
p_car_given_vaca <- p_car_and_vaca / p_vaca
p_car_given_vaca
```

```{r q1-a-4-chr, include=F, echo=F}
p_car_given_vaca_chr <- format_num(p_car_given_vaca)
p_car_given_vaca_chr
```

<response>
Thus, we find that $\Pr(\text{car=yes|vaca=yes})$ = `r p_car_given_vaca_chr`.
</response>

$$
\begin{aligned}
\Pr(\text{car=yes|vaca=yes})
& = \frac{\Pr(\text{vaca=yes,car=yes})}{\Pr(\text{vaca=yes})} \\
& = \frac{(`r p_car_and_vaca`)}{`r p_vaca_chr`} \\
& = `r p_car_given_vaca_chr`
\end{aligned}
$$

### b

We want to find $\Pr(\text{lott=win|vaca=yes})$. This can be calculated using Bayes' theorem as follows.

$$
\begin{aligned}
\Pr(\text{lott=win|vaca=yes})
&= \frac{\Pr(\text{vaca=yes|lott=win}) \Pr(\text{lott=win})}{\Pr(\text{vaca=yes})} \\
&= \frac{\Pr(\text{vaca=yes,lott=win})}{\Pr(\text{vaca=yes})}
\end{aligned}
$$

```{r q1-b-1}
p_vaca_and_lott <-
  states %>% 
  mutate(rn = row_number()) %>% 
  filter(vaca == 'yes', lott == 'win') %>% 
  summarise_at(vars(p), sum) %>% 
  pull(p)
p_vaca_and_lott

p_lott_given_vaca <- p_vaca_and_lott / p_vaca
p_lott_given_vaca
```

```{r q1-b-1-chr, include=F, echo=F}
p_lott_given_vaca_chr <- format_num(p_lott_given_vaca)
p_lott_given_vaca_chr 
```

$$
\begin{aligned}
\Pr\text{(lott=win|vaca=yes})
&= \frac{\Pr\text{(vaca=yes,lott=win})}{\Pr(\text{vaca=yes})} \\
&= \frac{(`r p_vaca_and_lott`)}{(`r p_vaca_chr`)} \\
&= `r p_lott_given_vaca_chr`.
\end{aligned}
$$

<response>
Thus, we have shown that $\Pr(\text{lott=win|vaca=yes})$ = `r p_lott_given_vaca_chr`.
</response>

### c

We want to find $\Pr(\text{grade=b|vaca=yes})$. This can be calculated using Bayes' theorem as follows.

$$
\begin{aligned}
\Pr(\text{grade=b|vaca=yes}) &
= \frac{\Pr(\text{vaca=yes|grade=b}) \Pr(\text{grade=b})}{\Pr(\text{vaca=yes})}
\end{aligned}
$$

```{r q1-c-1-debug, include=F, echo=F}
p_b_debug <- 
  states %>% 
  group_by(grade) %>% 
  summarise_at(vars(p), sum) %>% 
  ungroup() %>% 
  mutate_at(vars(p), ~(. / sum(.))) %>% 
  filter(grade == 'b') %>% 
  pull(p)
p_b_debug
```

We can calculate marginal probabilities for each possibility of `grade` given `vaca` in a similar way done before in part a (for just `vaca`)

```{r q1-c-1-pre}
p_vaca_given_grade <- 
  states %>% 
  group_by(vaca, grade) %>% 
  summarise_at(vars(p), sum) %>% 
  ungroup() %>% 
  group_by(grade) %>% 
  mutate_at(vars(p), ~(. / sum(.))) %>% 
  ungroup() %>% 
  arrange(grade, vaca)
p_vaca_given_grade
```

and isolate just the marginal probability that we are interested in.

```{r q1-c-1}
p_vaca_given_b <-
  p_vaca_given_grade %>% 
  filter(vaca == 'yes', grade == 'b') %>%
  pull(p)
p_vaca_given_b
```

```{r q1-c-1-chr, include=F, echo=F}
p_vaca_given_b_chr <- format_num(p_vaca_given_b, 1)
p_vaca_given_b_chr
```

We see that the probability of $\Pr(\text{vaca=yes|grade=b})$ = `r p_vaca_given_b_chr`, which is a term we need for the numerator of our Bayes' equation.

Finally, since we already have $\Pr(\text{grade=b})$ = `r p_b`, we can now calculate $\Pr(\text{grade=b|vaca=yes})$.

```{r q1-c-2}
p_b_given_vaca <- (p_vaca_given_b * p_b) / p_vaca
p_b_given_vaca
```

```{r q1-c-2-chr, include=F, echo=F}
p_b_given_vaca_chr <- format_num(p_b_given_vaca)
p_b_given_vaca_chr
```

<response>
Thus, we find that $\Pr(\text{grade=b|vaca=yes})$ = `r p_b_given_vaca_chr`.
</response>

$$
\begin{aligned}
\Pr(\text{grade=b|vaca=yes})
& = \frac{\Pr(\text{vaca=yes|grade=b}) \Pr(\text{grade=b})}{\Pr(\text{vaca=yes})} \\
& = \frac{(`r p_vaca_given_b_chr`) (`r p_b`)}{(`r p_vaca_chr`)} \\
& = `r p_b_given_vaca_chr`
\end{aligned}
$$

### d

We want to find $\Pr(\text{market=bear|vaca=yes})$. This can be calculated using Bayes' theorem as follows.

$$
\begin{aligned}
\Pr(\text{market=bear|vaca=yes}) &
= \frac{\Pr(\text{vaca=yes|market=bear}) \Pr(\text{market=bear})}{\Pr(\text{vaca=yes})}
\end{aligned}
$$

```{r q1-d-1-debug, include=F, echo=F}
p_bear_debug <- 
  states %>% 
  group_by(market) %>% 
  summarise_at(vars(p), sum) %>% 
  ungroup() %>% 
  filter(market == 'bear') %>% 
  pull(p)
p_bear_debug
```

```{r q1-d-1-pre}
states_bear <- 
  states %>% 
  group_by(vaca, market) %>% 
  summarise_at(vars(p), sum) %>% 
  ungroup() %>% 
  group_by(market) %>% 
  mutate_at(vars(p), ~(. / sum(.))) %>% 
  ungroup() %>% 
  arrange(market, vaca)
states_bear
```

```{r q1-d-1}
p_vaca_given_bear <-
  states_bear %>% 
  filter(vaca == 'yes', market == 'bear') %>%
  pull(p)
p_vaca_given_bear
```

```{r q1-d-1-chr, include=F, echo=F}
p_vaca_given_bear_chr <- format_num(p_vaca_given_bear, 1)
p_vaca_given_bear_chr
```

We see that the probability of $\Pr(\text{vaca=yes|market=bear})$ = `r p_vaca_given_bear_chr`, which is a term we need for the numerator of our Bayes' equation.

Finally, since we already have $\Pr(\text{market=bear})$ = `r p_bear`, we can now calculate $\Pr(\text{car=yes|vaca=yes})$.

```{r q1-d-2}
p_bear_given_vaca <- (p_vaca_given_bear * p_bear) / p_vaca
p_bear_given_vaca
```

```{r q1-d-2-chr, include=F, echo=F}
p_bear_given_vaca_chr <- format_num(p_bear_given_vaca)
p_bear_given_vaca_chr
```

<response>
Thus, we find that $\Pr(\text{market=bear|vaca=yes})$ = `r p_bear_given_vaca_chr`.
</response>

$$
\begin{aligned}
\Pr(\text{market=bear|vaca=yes})
& = \frac{\Pr(\text{vaca=yes|market=bear}) \Pr(\text{market=bear})}{\Pr(\text{vaca=yes})} \\
& = \frac{(`r p_vaca_given_bear_chr`)(`r p_bear`)}{(`r p_vaca_chr`)} \\
& = `r p_bear_given_vaca_chr`
\end{aligned}
$$

# 2. Trials until Fourth Success.

## Instructions

<instructions>
The number of failures until the fourth success in a series of independent trials ...
</instructions>

## Response

Given $X = (X_1, \dots, X_n)$ is a sample from $\mathcal{NB}(m, p)$ and $p \sim \mathcal{Be}(a, b)$, the posterior for $p$ follows a $\mathcal{Be}(a + m n, b + \sum_{i=1}^n x_i)$ distribution. The proof of this relationship is provided in an an exercise from earlier in the semester. It goes as follows.

<i>Proof.</i>

Recall that the pmf of the negative binomial distribution $\mathcal{NB}(m, p)$ (which model the number of failures before the $m$th success in $n$ Bernoulli experiments) is given by

$$
\begin{aligned}
f(x)=\left(\begin{array}{c}{m+x-1} \\ {x}\end{array}\right) p^{m}(1-p)^{x}, x=0,1,2, \ldots
\end{aligned}
$$

and that the pdf of the Beta distribution $\mathcal{Be}(a, b)$ is proportional to

$$
\begin{aligned}
f(x) = \frac{1}{\operatorname{B}(x)} x^{a-1} (1 - x) ^ {b - 1}, 0 \leq x \leq 1
\end{aligned}
$$

(where $\operatorname{B}$ is the beta function).

If the random variable $X$ comes from $\mathcal{NB}(m, p)$, then the likelihood $\mathcal{L}(x)$ is proportional to $p^{m n} (1 - p) ^ {\sum_{i=1}^n x_i}$; and if the prior $\pi(p)$ comes form $\mathcal{Be}(a, b)$, then $\pi(p) \propto p^{a-1} (1 - p) ^ {b - 1}$. This results in a posterior proportional to $p^{m n+a-1}(1-p) ^{\sum_{i=1}^{n} x_{i}+b-1}$.

$$
\begin{array}{c*}
\text{likelihood} & \times & \text{prior} & \propto & \text{posterior} \\
p^{m n}(1-p) ^ {\sum_{i=1}^{n} x_{i}} & \times & p^{a-1}(1-p)^{b-1} & \propto & p^{m n+a-1}(1-p) ^{\sum_{i=1}^{n} x_{i}+b-1}.
\end{array}
$$

This is a kernel of the beta distribution $\mathcal{Be}(a+mn, b + \sum_{i=1}^n x_i)$. $\square$

For this problem set-up, note that $m = 4$, $n = 11$, and $x = [5, 2, 2, 0, 1, 4, 3, 1, 2, 5, 0, 7, 1]$.

In the subsections that follow, I calculate (1) the Bayes estimator $p_{\text{Bayes}}$, (2) the 95% credible set for $p$, and (3) the posterior probability of hypothesis $H : p \geq 0.8$ (along with several other things not explicitly required, such as the Bayes factor $B_{01}$). Each of these concepts deserve some brief discussion beforehand.


Regarding (1), the Bayes estimator $p_{\text{Bayes}}$ is simply the expected value of $p$ under the posterior distribution, i.e. the posterior mean. The expected value of arbitrary random variable $x$ for a Beta distribution is $\operatorname{E}[x] = \frac{a}{a+b}$. Thus, for this setup, $p_{\text{Bayes}} = \operatorname{E}[p_{\text{posterior}}] = \frac{a+mn}{b + \sum_{i=1}^n x_i}$.

Regarding (2), a credible set $C = [L, U]$ defines the parameter space of the posterior distribution with $1-\alpha$ credibility. (In this problem, we let $\alpha=0.05$). An equitailed credible set is just one form of a credible set. Formally, it is calculated as follows.

$$
\begin{array}{c}
\int_{-\infty}^{L} \pi(p|x) dp \leq \frac{\alpha}{2}, 
\int_{U}^{+\infty} \pi(p|x) dp \leq \frac{\alpha}{2} \\
\text{ s.t. }
\Pr(p \in \left[L, U\right] | X) \geq 1 - \alpha
\end{array}
$$

Regarding (3), to test the one-sided (null) hypothesis $H_0 : p \geq p_{split}$ (here, $p_{split} = 0.8$--against the (alternative) alternative $H_1 : p < p_{split}$, we simply choose the hypothesis with larger posterior probability. We can go further by calculating the Bayes Factor $B_{01}$ in favor of $H_0$ (or the Bayes Factor $B_{10}$ in favor of $H_1$) and calibrate our deduction in favor or/against $H_0$ using tables like the one shown in lecture.

```{r q2-bayes-calibration-table, include=T, echo=F, eval=T, fig.width=5, fig.asp=0.75}
knitr::include_graphics('bayes_calibration.png')
```

The following code implements all parts of the instructions. Specifically `do_q2()` calls other functions to compute the required components---(1) `compute_beta_mu()` for $p_{\text{Bayes}}$, `compute_equi_credible_set_beta()` for the 95% credible set, and (3) `compute_p_h1()` for the hypothesis test. Discussion of the findings for each set of priors is included in the following subsections.

```{r q2-0}
set.seed(42)
m <- 4
x <- c(5, 2, 2, 0, 1, 4, 3, 5, 0, 7, 1)
n <- length(x) # This is 11.
p_split <- 0.8
# p_split <- 0.6

.compute_a_post <- function(a, m, n) {
  a + m * n
}

compute_a_post <- function(a, .m = m, .n = n) {
  .compute_a_post(a, .m, .n)
}

.compute_b_post <- function(b, x) {
  b + sum(x)
}

compute_b_post <- function(b, .x = x) {
  .compute_b_post(b = b, x = .x)
}

compute_beta_mu <- function(a, b) {
  a / (a + b)
}

compute_equi_credible_set_beta <- function(a, b, level = 0.95) {
  q_buffer <- (1 - level) / 2
  q_l <- (1 - level) - q_buffer
  q_u <- level + q_buffer
  res <-
    c(
      l = qbeta(q_l, a, b),
      u = qbeta(q_u, a, b)
    )
  res
}

# .compute_equi_credible_set <- function(x, level = 0.95) {
#   q_buffer <- (1 - level) / 2
#   q_l <- (1 - level) - q_buffer
#   q_u <- level + q_buffer
#   res <-
#     c(
#       l = quantile(x, q_l),
#       u = quantile(x, q_u)
#     )
#   res
# }
# 
# compute_equi_credible_set <- function(.x = x) {
#   .compute_equi_credible_set(x = .x)
# }

.compute_p_h1 <- function(a, b, p) {
  pbeta(p, a, b, lower.tail = TRUE)
}

compute_p_h1 <- function(a, b, .p_split = p_split) {
  .compute_p_h1(a, b, .p_split)
}

do_q2 <- function(a_0, b_0) {
  a_1 <- compute_a_post(a_0)
  b_1 <- compute_b_post(b_0)
  mu_prior <- compute_beta_mu(a_0, b_0)
  mu_post <- compute_beta_mu(a_1, b_1)
  # credible_set <- compute_equi_credible_set()
  credible_set_beta <- compute_equi_credible_set_beta(a_1, b_1)
  p_prior_h1 <- compute_p_h1(a_0, b_0)
  p_prior_h0 <- 1 - p_prior_h1
  p_post_h1 <- compute_p_h1(a_1, b_1)
  p_post_h0 <- 1 - p_post_h1
  b_01_num <- p_post_h0 / p_post_h1 
  b_01_den <- p_prior_h0 / p_prior_h1
  b_01 <- b_01_num / b_01_den
  b_10_num <- p_post_h1 / p_post_h0
  b_10_den <- p_prior_h1 / p_prior_h0
  b_10 <- b_10_num / b_10_den
  res <- 
    list(
      # a_prior = a_0,
      # b_prior = b_0,
      a_post = a_1,
      b_post = b_1,
      mu_prior = mu_prior,
      mu_post = mu_post,
      credible_set_l = credible_set_beta['l'],
      credible_set_u = credible_set_beta['u'],
      p_prior_h0 = p_prior_h0,
      p_prior_h1 = p_prior_h1,
      p_post_h0 = p_post_h0,
      p_post_h1 = p_post_h1,
      # b_01 = b_01,
      # b_10 = b_10,
      b_01_log10 = log10(b_01),
      b_10_log10 = log10(b_10)
    )
  res
}
```

```{r q2-0-extra, include=F, echo=F}
n_success <- n * m
n_fail <- sum(x)
n_total <- n_success + n_fail
mu_actual <- n_success / n_total
mu_actual
```

```{r q2-0-extra-chr, include=F, echo=F}
mu_actual_chr <- format_num(mu_actual)
mu_actual_chr
```


### a

```{r q2-a-1-prep, include=F, echo=F}
a_0_a <- 1
b_0_a <- a_0_a
```

The results in this subsection are just for the priors $a = b$ = `r a_0_a`. (The following subsections show the results for the other sets of priors.)

```{r q2-a-1-prep-show, ref.label='q2-a-1-prep'}
```


```{r q2-a-1}
res_a <- do_q2(a_0_a, b_0_a)
res_a
```

The `R` package `{R2OpenBUGS}` was also used to computer the posterior probabilities. See the code and output below.

```{r q2-a-2, eval=F}
model <- function() {
  p ~ dbeta(a, b)
  for(i in 1:n){
    x[i] ~ dnegbin(p, m)
  }
  p_h_0 <- step(p - 0.8)
  p_h_1 <- step(0.8 - p)
}
data <- list(a = 0.5, b = 0.5, m = 4, n = 11, x = c(5, 2, 2, 0, 1, 4, 3, 5, 0, 7, 1))
inits <- NULL
params <- c('p_h_0', 'p_h_1', 'p')
res_sim <-
  R2OpenBUGS::bugs(
    data = data,
    inits = inits,
    model.file = model,
    parameters.to.save = params,
    DIC = FALSE,
    # debug = TRUE,
    n.chains = 1,
    n.iter = 10000,
    n.burnin = 1000
  )
res_sim$summary
```

```{r q2-a-2-hide, include=F, echo=F, eval=F}
clipr::write_clip(knitr::kable(res_sim$summary))
```


|      |      mean|        sd|   2.5%|    25%|    50%|      75%|     97.5%|
|:-----|---------:|---------:|------:|------:|------:|--------:|---------:|
|p_h_0 | 0.0000000| 0.0000000| 0.0000| 0.0000| 0.0000| 0.000000| 0.0000000|
|p_h_1 | 1.0000000| 0.0000000| 1.0000| 1.0000| 1.0000| 1.000000| 1.0000000|
|p     | 0.5922603| 0.0561095| 0.4783| 0.5552| 0.5936| 0.630625| 0.7009025|


<response>
For (1), we find that $p_{\text{Bayes}}$ = `r format_num(res_a$mu_post, 4)` (corresponding to `mu_post` in `res_a`).<br/>For (2), we find that the 95% credible set $C$ = [`r format_num(res_a$credible_set_l, 4)`, `r format_num(res_a$credible_set_u, 4)`] (corresponding to `credible_set_l` and `credible_set_u` respectively).<br/>For (3), we find that the posterior probability of the hypothesis $H_1 : p$ < `r p_split` is $p_{H_1}$ = `r format_num(res_a$p_post_h1, 4)` (corresponding to `p_post_h1`), which is much larger than that for the the hypothesis $H_0 : p \geq$ `r p_split`), $p_{H_0}$ = `r format_num(res_a$p_post_h0, 4)` (corresponding to `p_post_h0`).
</response>

Furthermore, we note that the log of the Bayes Factor $B_{10}$ (in favor of $H_1$ compared to $H_0$) provides decisive evidence against $H_0$---or, equivalently, in favor of $H_1$---because $\log_{10}(B_{10}) > 2$ (and because $\log_{10}(B_{01}) < 0.5$). Also, we observe that the upper bound of the equitailed 95% credible interval---`r format_num(res_a$credible_set_u, 4)`---is smaller than the hypothesis test probability, so it is not unsurprising that the conclusion to be made is so definitive.

### b

```{r q2-b-1-prep, include=F, echo=F}
a_0_b <- 0.5
b_0_b <- a_0_b
```

See the code and output below for the same calculations performed in part a for priors $a = b$ = `r a_0_b`

```{r q2-b-1-prep-show, ref.label='q2-b-1-prep'}
```

```{r q2-b-1}
res_b <- do_q2(a_0_b, b_0_b)
res_b
```

Again, `{R2OpenBUGS}` was used to check the results. (The code is the same as in part a, with the exception of `a` and `b` variables in the data.)

|      |      mean|        sd|      2.5%|    25%|    50%|    75%|    97.5%|
|:-----|---------:|---------:|---------:|------:|------:|------:|--------:|
|p_h_0 | 0.0000000| 0.0000000| 0.0000000| 0.0000| 0.0000| 0.0000| 0.000000|
|p_h_1 | 1.0000000| 0.0000000| 1.0000000| 1.0000| 1.0000| 1.0000| 1.000000|
|p     | 0.5936116| 0.0563999| 0.4795975| 0.5563| 0.5949| 0.6323| 0.702805|


<response>
For (1), we find that $p_{\text{Bayes}}$ = `r format_num(res_b$mu_post, 4)`.<br/>For (2), we find that the 95% credible set $C$ = [`r format_num(res_b$credible_set_l, 4)`, `r format_num(res_b$credible_set_u, 4)`].<br/>F For (3), we find that the posterior probability of the hypothesis $H_1$ is $p_{H_1}$ = `r format_num(res_b$p_post_h1, 4)`, which is much larger than that for the the hypothesis $H_0$, $p_{H_0}$ = `r format_num(res_b$p_post_h0, 4)`.
</response>

The additional observations made about the Bayes Factor and the upper credible set bound in part a also apply here.

### c

```{r q2-c-1-prep, include=F, echo=F}
a_0_c <- 9
b_0_c <- 1
```

See the code and output below for the same calculations performed in part a for priors $a$ = `r a_0_c` and $b$ = `r b_0_c`.

```{r q2-c-1-prep-show, ref.label='q2-c-1-prep'}
```

```{r q2-c-1}
res_c <- do_q2(a_0_c, b_0_c)
res_c
```

And the output from `{R2OpenBUGS}` is shown below.

|      |      mean|        sd|      2.5%|    25%|     50%|    75%|     97.5%|
|:-----|---------:|---------:|---------:|------:|-------:|------:|---------:|
|p_h_0 | 0.0001111| 0.0105409| 0.0000000| 0.0000| 0.00000| 0.0000| 0.0000000|
|p_h_1 | 0.9998889| 0.0105409| 1.0000000| 1.0000| 1.00000| 1.0000| 1.0000000|
|p     | 0.6312816| 0.0521394| 0.5269975| 0.5965| 0.63235| 0.6676| 0.7291025|


<response>
For (1), we find that $p_{\text{Bayes}}$ = `r format_num(res_c$mu_post, 4)`.<br/>For (2), we find that the 95% credible set $C$ = [`r format_num(res_c$credible_set_l, 4)`, `r format_num(res_c$credible_set_u, 4)`].<br/>For (3), we find that the posterior probability of the hypothesis $H_1$ is $p_{H_1}$ = `r format_num(res_c$p_post_h1, 4)`, which is much larger than that for the the hypothesis $H_0$, $p_{H_0}$ = `r format_num(res_c$p_post_h0, 4)`.
</response>

Again, the additional observations made about the Bayes Factor and the upper credible set bound in part a also apply here.

Thus, for all three prior sets, we find that evidence in favor of $H_1 : p$ < `r p_split` is decisive. Really, this is unsurprising. Note that the observed mean is `r mu_actual_chr`.

```{r q2-0-extra-show, ref.label='q2-0-extra'}
```

Then, given the flat priors of (a) and (b), we should have expected that the posterior mean would not be much different. And the prior set for (c) is not all that "strong" either, so we should have expected similar results.

# 3. Penguins.

## Instructions

<instructions>
A researcher is interested in testing ...
</instruction>

## Response


```{r q3-1-prep, include=F, echo=F}
set.seed(42)
y <- c(41, 44, 43, 47, 43, 46, 45, 42, 45, 45, 43, 45, 47, 40)
y_sum <- sum(y)
y_sum
n_obs <- length(y)
n_obs

n_burnin <- 1000L
n_mcmc <- 10000L + n_burnin

mu_0 <- 45
tau_0 <- 1 / (2^2)
a_0 <- 4
b_0 <- 2
```


In what follows I derive the full conditional expressions for $\mu$ and $\tau$, closely following the guide provided by the Gibbs handout from class.

Per the instructions, we make the assumption that the measurements $y_i \dots y_n$ come from the normal distribution $\mathcal{N}(\mu, 1/\tau)$. (Note that $n$ = `r n_obs` here.) Furthermore, we assume $\mu \sim \mathcal{N}(\mu_0, 1 / \tau_0)$ (where $\mu_0$ = `r mu_0` and $\tau_0$ = `r 1 / tau_0`, per the instructions), and the precision parameter $\tau \sim \mathcal{Ga}(a_0, b_0)$ (where $a_0$ = `r a_0` and $b_0$ = `r b_0`).

Now, in preparation of expressing the joint distribution, we define the likelihood of $\mu$ as

$$
\mathcal{L}(\mu | y_1, \ldots y_n)=\prod_{i=1}^{n} f(y_i| \mu, \tau) \propto \tau^{n/2} \exp \left\{-\frac{\tau}{2} \sum_{i=1}^n\left(y_i-\mu\right)^{2}\right\}.
$$

Thus, the joint distribution is


$$
\begin{aligned}
f(y, \mu, \tau) &=\left\{\prod_{i=1}^{n} f\left(y_{i} | \mu, \tau\right)\right\} \pi(\mu) \pi(\tau) \\
& \propto \tau^{n / 2} \exp \left\{-\frac{\tau}{2} \sum_{i=1}^{n}\left(y_{i}-\mu\right)^{2}\right\} \exp \left\{-\left(\mu-\mu_{0}\right)^{2} / 2\right\} \tau^{a_0-1} \exp \{-b_0 \tau\}
\end{aligned}
$$

After selecting the terms from the joint distribution expression that contain $\mu$ and normalizing, we find that

$$
\begin{aligned}
\pi(\mu | \tau, y)
& \propto \exp \left\{-\frac{\tau}{2} \sum_{i=1}^{n}\left(y_{i}-\mu\right)^{2}\right\} \exp \left\{-\frac{\tau_{0}}{2}\left(\mu-\mu_{0}\right)^{2}\right\} \\
& \propto \exp \left\{-\frac{1}{2}\left(\tau_{0}+n \tau\right)\left(\mu-\frac{\tau \sum y_{i}+\tau_{0} \mu_{0}}{\tau_{0}+n \tau}\right)^{2}\right\}
\end{aligned}
$$
which is the kernel for the $\mathcal{N}\left(\frac{\tau \sum y_{i}+\tau_{0} \mu_{0}}{\tau_{0}+n \tau}, \frac{1}{\tau_{0}+n \tau}\right)$ distribution. Plugging in the provided numbers for $y_i, \dots, y_n$, $\mu_0$, $\tau_0$ and $n$, we can write the full conditional for $\mu$ explicitly as

$$
\begin{aligned}
\mu | \tau, y
& \sim \mathcal{N}\left(\frac{\tau (616) + (2^{-2}) (45)}{(2^{-2})+(14) \tau}, \frac{1}{(2^{-2})+(14)\tau}\right) \\
& \sim \mathcal{N}\left(\frac{616 \tau + 11.25)}{0.25 + 14 \tau},\frac{1}{0.25 + 14\tau}\right)
\end{aligned}
$$


Next, we can derive the full conditional for $\tau$ as follows.

$$
\begin{aligned}
\pi(\tau | \mu, y)
& \propto \tau^{n / 2} \exp \left\{-\frac{\tau}{2} \sum_{i=1}^{n}\left(y_{i}-\mu\right)^{2}\right\} \tau^{a_0-1} \exp \{-b_0 \tau\} \\
& = \tau^{n / 2+a_0-1} \exp \left\{-\tau\left[b+\frac{1}{2} \sum_{i=1}^{n}\left(y_{i}-\mu\right)^{2}\right]\right\}
\end{aligned}
$$

which is a kernel for the $\mathcal{Ga}\left(a_0+\frac{n}{2}, b_0+\frac{1}{2} \sum_{i=1}^{n}\left(y_{i}-\mu\right)^{2}\right)$ distribution. We can write this explicitly as

$$
\begin{aligned}
\tau | \mu, y
& \sim \mathcal{Ga}\left((4) + \frac{(14)}{2}, (2) + \frac{1}{2} \sum_{i=1}^{(14)} (y_i - \mu)^2\right) \\
& \sim \mathcal{Ga}\left(18, 2 + \frac{1}{2} \sum_{i=1}^{14} (y_i - \mu)^2\right)
\end{aligned}
$$


Please see the code and its output below for the implementation of a Gibbs sampler for this problem. Discussion of the output follows. <b>IMPORTANT: Note that I use variable suffixes `_1` and `_2` (starting with `do_mcmc_gibbs*()` to differentiate between calculations where I define $\tau$ as the `rate` parameter in R's `rgamma()` function (which seems to make a lot more sense to me given the prior and despite the instructions indicating that it is the scale parameter) and calculations where $\tau$ is defined as the `scale` parameter (which seems to make less sense to me).</b>

```{r q3-1-prep-show, ref.label='q3-1-prep'}
```


```{r q3-1}
.compute_mu_new <- function(mu_i, tau_i, mu_0, tau_0, y_sum, n_obs) {
  mu_tau_0 <- mu_0 * tau_0
  mu_rnorm_num <- tau_i * y_sum + mu_tau_0
  mu_rnorm_den <- tau_0 + n_obs * tau_i
  mu_rnorm <- mu_rnorm_num / mu_rnorm_den
  sigma2_rnorm <- 1 / (tau_0 + n_obs * tau_i)
  sigma_rnorm <- sqrt(sigma2_rnorm)
  rnorm(1, mu_rnorm, sigma_rnorm)
}

n_obs <- length(y)
compute_mu_new <-
  function(mu_i,
           tau_i,
           .mu_0 = mu_0,
           .tau_0 = tau_0,
           .y_sum = y_sum,
           .n_obs = n_obs) {
    .compute_mu_new(
      mu_i,
      tau_i,
      mu_0 = .mu_0,
      tau_0 = .tau_0,
      y_sum = .y_sum,
      n_obs = .n_obs
    )
  }

.compute_tau_new <- function(mu_new, y, a_0, b_0, n_obs, scale = TRUE) {
  shape_rgamma <- a_0 + 0.5 * n_obs
  z_rgamma  <- b_0 + 0.5 * sum((y - mu_new) ^ 2)
  if(scale) {
    res <- rgamma(1, shape = shape_rgamma, scale = z_rgamma) 
  } else {
    res <- rgamma(1, shape = shape_rgamma, rate = z_rgamma)
  }
  res
}

compute_tau_new <-
  function(mu_new,
           .y = y,
           .a_0 = a_0,
           .b_0 = b_0,
           .n_obs = n_obs,
           scale = TRUE) {
    .compute_tau_new(
      mu_new = mu_new,
      y = .y,
      a_0 = .a_0,
      b_0 = .b_0,
      n_obs = .n_obs,
      scale = scale
    )
  }

mu_init <- mean(y)
tau_init <- 1 / sd(y)
```

```{r q3-2}
is_likeinteger <- function(x, tol = .Machine$double.eps^0.5) {
  abs(x - round(x)) < tol
}

.do_mcmc_gibbs <-
  function(n_mcmc,
           n_burnin,
           mu_init,
           tau_init,
           scale = TRUE) {
    stopifnot(is_likeinteger(n_mcmc))
    if (!is.null(n_burnin)) {
      stopifnot(is_likeinteger(n_burnin))
      stopifnot(n_burnin < n_mcmc)
    }
    stopifnot(is.numeric(mu_init))
    stopifnot(is.numeric(tau_init))
    cols_mat_mcmc <- c('mu', 'tau')
    mat_mcmc <- matrix(nrow = n_mcmc, ncol = length(cols_mat_mcmc))
    
    colnames(mat_mcmc) <- cols_mat_mcmc
    mu_i <- mu_init
    tau_i <- tau_init
    y_sum <- sum(y)
    n_obs <- length(y)
    for (i in 1:n_mcmc) {
      mu_new <- compute_mu_new(mu_i = mu_i, tau_i = tau_i)
      tau_new <- compute_tau_new(mu_new = mu_new, scale = scale)
      mat_mcmc[i,] <- c(mu_new, tau_new)
      mu_i <- mu_new
      tau_i <- tau_new
    }
    res_mcmc <-
      mat_mcmc %>% 
      as_tibble() %>% 
      mutate(idx = row_number()) %>% 
      select(idx, everything())
    
    if (!is.null(n_burnin)) {
      idx_slice <- (n_burnin + 1):nrow(mat_mcmc)
      res_mcmc <- res_mcmc %>% slice(idx_slice)
    }
    
    res_mcmc
  }

do_mcmc_gibbs_1 <-
  function(...,
           .n_mcmc = n_mcmc,
           .n_burnin = n_burnin,
           .mu_init = mu_init,
           .tau_init = tau_init, 
           .scale = FALSE) {
    .do_mcmc_gibbs(
      n_mcmc = .n_mcmc,
      n_burnin = .n_burnin,
      mu_init = .mu_init,
      tau_init = .tau_init,
      scale = .scale,
      ...
    )
  }

do_mcmc_gibbs_2 <-
  function(...,
           .n_mcmc = n_mcmc,
           .n_burnin = n_burnin,
           .mu_init = mu_init,
           .tau_init = tau_init,
           .scale = TRUE) {
    .do_mcmc_gibbs(
      n_mcmc = .n_mcmc,
      n_burnin = .n_burnin,
      mu_init = .mu_init,
      tau_init = .tau_init,
      scale = .scale,
      ...
    )
  }
```

```{r q3-3}
# Treating tau as `rate` parameter.
res_mcmc_gibbs_1 <- do_mcmc_gibbs_1()
res_mcmc_gibbs_1
# Treating tau as `scale` parameter.
res_mcmc_gibbs_2 <- do_mcmc_gibbs_2()
res_mcmc_gibbs_2
```

```{r q3-4, include=F, echo=F, eval=F}
res_mcmc_gibbs_1 %>% summarise_all(mean)
res_mcmc_gibbs_2 %>% summarise_all(mean)
```

```{r q3-viz-1-prep, include=F, echo=F}
path_viz_1 <- 'viz_mcmc_gibbs_1.png'
path_viz_2 <- 'viz_mcmc_gibbs_2.png'
w_viz <- 6
h_viz <- 4
eval_viz_1 <- !fs::file_exists(path_viz_1)
eval_viz_2 <- !fs::file_exists(path_viz_2)
```

```{r viz-funcs-0}
theme_custom <- function(...) {
  theme_minimal() +
    theme(
      legend.position = 'bottom',
      legend.title = element_blank(),
      axis.title.x = element_text(hjust = 1),
      axis.title.y = element_text(hjust = 1),
      ...
    )
}
```

```{r q3-viz-1, eval=eval_viz_1}
viz_mcmc_gibbs_1 <-
  res_mcmc_gibbs_1 %>% 
  gather(key = 'key', value = 'value', -idx) %>% 
  ggplot() +
  aes(x = value) +
  geom_histogram() +
  scale_y_continuous(labels = scales::comma) +
  theme_custom() +
  facet_wrap(~key, scales = 'free') +
  labs(
    caption = 'tau treated as `rate` paremeter in R\'s `rgamma()` function.',
    x = NULL,
    y = 'Frequency'
  )
```

```{r q3-viz-1-export, include=F, echo=F, eval=eval_viz_1}
teproj::export_ext_png(
  viz_mcmc_gibbs_1,
  path = path_viz_1,
  units = 'in',
  width = w_viz,
  height = h_viz
)
```

```{r q3-viz-1-show-1, eval=F}
viz_mcmc_gibbs_1
```

```{r q3-viz-1-show-2, include=T, echo=F}
knitr::include_graphics(path_viz_1)
```

```{r q3-viz-2, eval=eval_viz_2}
viz_mcmc_gibbs_2 <-
  res_mcmc_gibbs_2 %>% 
  gather(key = 'key', value = 'value', -idx) %>% 
  ggplot() +
  aes(x = value) +
  geom_histogram() +
  scale_y_continuous(labels = scales::comma) +
  theme_custom() +
  facet_wrap(~key, scales = 'free') +
  labs(
    caption = 'tau treated as `scale` paremeter in R\'s `rgamma()` function.',
    x = NULL,
    y = 'Frequency'
  )
```

```{r q3-viz-2-export, include=F, echo=F, eval=eval_viz_2}
teproj::export_ext_png(
  viz_mcmc_gibbs_2,
  path = path_viz_2,
  units = 'in',
  width = w_viz,
  height = h_viz
)
```

```{r q3-viz-2-show-1, eval=F}
viz_mcmc_gibbs_2
```

```{r q3-viz-2-show-2, include=T, echo=F}
knitr::include_graphics(path_viz_2)
```

As a quick check of the validity of our results, we can compare the posterior mean $\hat{\mu}$ (from the Gibbs sampling results) with those of the observations $y$ and the prior $\mu_0$.


```{r q3-5}
mcmc_gibb_mu_mean_1 <-
  res_mcmc_gibbs_1 %>% 
  summarise_at(vars(mu), ~mean(.)) %>% 
  pull(mu)
mcmc_gibb_mu_mean_1

mcmc_gibb_mu_mean_2 <-
  res_mcmc_gibbs_2 %>% 
  summarise_at(vars(mu), ~mean(.)) %>% 
  pull(mu)
mcmc_gibb_mu_mean_2

y_mean <- mean(y)
y_mean
```

```{r q3-5-extra, include=F, echo=F}
2 + 0.5 * sum((y - y_mean)^2)
```

```{r q3-5-chr, include=F, echo=F}
mcmc_gibb_mu_mean_1_chr <- format_num(mcmc_gibb_mu_mean_1, digits = 4)
mcmc_gibb_mu_mean_1_chr
mcmc_gibb_mu_mean_2_chr <- format_num(mcmc_gibb_mu_mean_2, digits = 4)
mcmc_gibb_mu_mean_2_chr

y_mean_chr <- format_num(y_mean, digits = 0)
y_mean_chr
```

We find that $\hat{\mu}$ = `r mcmc_gibb_mu_mean_1_chr`, which is between the observed mean $\hat{y}$ = `r y_mean_chr` and the prior $\mu_0$ = `r mu_0`. This is what we should have expected. (Note that $\hat{\mu}$ = `r mcmc_gibb_mu_mean_2_chr` in the second set of calculations, which is a big reason why I believe that tau should not be treated as the `scale` parameter for R's `rgamma()` function.)

### a

```{r q3-a-1-prep, include=F, echo=F}
mu_split <- 45
```


To approximate the posterior probability $p_{H_{0}}$ of the hypothesis $H_0 : \mu$ < `r mu_split`, we calculate the fraction of samples where this condition is met. (Note that $\tau$ does not matter here, so we don't show calculations for the two methods of calculating the posterior of $\tau$.)

```{r q3-a-1-prep-show, ref.label='q3-a-1-prep'}
```

```{r q3-a-1}
h_0_1 <-
  res_mcmc_gibbs_1 %>% 
  mutate(h_0 = ifelse(mu < mu_split, 1, 0)) %>% 
  summarise(n_h_0 = sum(h_0), n = n()) %>% 
  mutate(frac = n_h_0 / n) %>% 
  select(n_h_0, frac)
h_0_1
```

```{r q3-a-1-post, include=F, echo=F}
p_h_0_1 <- h_0_1 %>% pull(frac)
p_h_0_1
p_h_0_1_chr <- p_h_0_1 %>% format_num(3)
p_h_0_1_chr
p_h_0_1_pct_chr <- (p_h_0_1 * 100) %>% format_num(1)
p_h_0_1_pct_chr
```
<response>
We find that the posterior probability is $p_{H_{0}}$ = `r p_h_0_1_chr` (i.e. `r p_h_0_1_pct_chr`%).
</response>This very large probability is illustrated by the histogram from above, which shows a large majority of $\mu$ samples with values less than `r mu_split`.

Additionally, we may calculate the Bayes Factor $B_{01}$ in favor of hypothesis $H_0$.

```{r q3-a-2}
p_h_0_1 <- h_0_1 %>% pull(frac)
p_h_1_1 <- 1 - p_h_0_1
b_01_1 <- p_h_0_1 / p_h_1_1
b_01_1_log10 <- log10(b_01_1)
b_01_1_log10
```

```{r q3-a-2-post, include=F, echo=F}
b_01_1_log10_chr <- format_num(b_01_1_log10)
b_01_1_log10_chr
```

We deduce that there is strong evidence against the alternative hypothesis $H_1 \geq$ `r mu_split`---or equivalently, in favor of $H_0$---because $1.5 < \log_{10}(B_{01})$ = `r b_01_1_log10_chr` $\leq 2$.


### b

We can calculate the 95% equitailed credible set for $\tau$ (trying both methods, as described before) as follows.

```{r q3-b-1}
equi_credible_set_1 <-
  res_mcmc_gibbs_1 %>% 
  summarise_at(
    vars(tau), 
    list(
      l = ~quantile(., 0.025), 
      u = ~quantile(., 0.975)
    ))
equi_credible_set_1

equi_credible_set_2 <-
  res_mcmc_gibbs_2 %>% 
  summarise_at(
    vars(tau), 
    list(
      l = ~quantile(., 0.025), 
      u = ~quantile(., 0.975)
    ))
equi_credible_set_2
```

```{r q3-b-2, include=F, echo=F}
credible_set_1 <- equi_credible_set_1 %>% select(l, u) %>% c()
credible_set_1
credible_set_2 <- equi_credible_set_2 %>% select(l, u) %>% c()
credible_set_2
```

```{r q3-b-3, include=F, echo=F}
credible_set_1_chr <- format_num(credible_set_1)
credible_set_1_chr
credible_set_2_chr <- format_num(credible_set_2)
credible_set_2_chr
```

<response>
We find that the 95% equitailed credible set is [`r credible_set_1_chr[1]`, `r credible_set_1_chr[2]`].
</response>(For the second set of $\tau$ calculations, this is [`r credible_set_2_chr[1]`, `r credible_set_2_chr[2]`].)
If we had exact posterior estimates for $a$ and $b$ for the $\mathcal{Ga}$ posterior distribution, we could have possibly used `R`'s `qgamma()` function to compute the interval.
